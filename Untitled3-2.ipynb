{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 229,
      "metadata": {
        "id": "_YntCWBQssb1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch transformers timm einops bitsandbytes accelerate -q\n",
        "!pip install peft -q\n",
        "!pip install flash_attn --no-build-isolation -q\n",
        "!pip install hf_transfer -q\n",
        "!pip install Ipython -q\n",
        "!pip install wandb -q\n",
        "!pip install torchviz -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 197,
      "metadata": {
        "id": "jaRNtlvEtjdJ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import requests\n",
        "import os\n",
        "\n",
        "os.environ['HF_HUB_ENABLE_HF_TRANSFER'] = '1'\n",
        "os.environ['HF_TOKEN'] = '//'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 198,
      "metadata": {
        "id": "hB-faASLttlx"
      },
      "outputs": [],
      "source": [
        "train_df = pd.read_csv(\"/home/vishwa/amzn-ml/student_resource 3/dataset/train.csv\")\n",
        "test_df = pd.read_csv('/home/vishwa/amzn-ml/student_resource 3/dataset/test.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 199,
      "metadata": {
        "id": "MFnd46NouCfY"
      },
      "outputs": [],
      "source": [
        "entity_unit_map = {\n",
        "    'width': ['centimetre', 'foot', 'inch', 'metre', 'millimetre', 'yard'],\n",
        "    'depth': ['centimetre', 'foot', 'inch', 'metre', 'millimetre', 'yard'],\n",
        "    'height': ['centimetre', 'foot', 'inch', 'metre', 'millimetre', 'yard'],\n",
        "    'item_weight': ['gram',\n",
        "        'kilogram',\n",
        "        'microgram',\n",
        "        'milligram',\n",
        "        'ounce',\n",
        "        'pound',\n",
        "        'ton'],\n",
        "    'maximum_weight_recommendation': ['gram',\n",
        "        'kilogram',\n",
        "        'microgram',\n",
        "        'milligram',\n",
        "        'ounce',\n",
        "        'pound',\n",
        "        'ton'],\n",
        "    'voltage': ['kilovolt', 'millivolt', 'volt'],\n",
        "    'wattage': ['kilowatt', 'watt'],\n",
        "    'item_volume': ['centilitre',\n",
        "        'cubic foot',\n",
        "        'cubic inch',\n",
        "        'cup',\n",
        "        'decilitre',\n",
        "        'fluid ounce',\n",
        "        'gallon',\n",
        "        'imperial gallon',\n",
        "        'litre',\n",
        "        'microlitre',\n",
        "        'millilitre',\n",
        "        'pint',\n",
        "        'quart']\n",
        "}\n",
        "\n",
        "allowed_units = {unit for entity in entity_unit_map for unit in entity_unit_map[entity]}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 200,
      "metadata": {
        "id": "wQEO4L90tKQk"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "# from datasets import load_dataset\n",
        "\n",
        "class AmazonDataset(Dataset):\n",
        "    def __init__(self, train_df):\n",
        "        self.data = train_df\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # print(idx)\n",
        "        sample = self.data.loc[idx, :]\n",
        "        # print(sample['image_link'])\n",
        "        return {\n",
        "            \"image\": Image.open('/home/vishwa/amzn-ml/student_resource 3/train_images/' + sample['image_link'].split('/')[-1]),\n",
        "            # \"image\": Image.open(requests.get(sample['image_link'], stream=True)), # Should be a PIL image\n",
        "            \"qa\": [\n",
        "                {\n",
        "                    \"question\": f\"Extract the {sample['entity_name']} from the text and only use these units {', '.join(entity_unit_map[train_df['entity_name'][0]])}\",\n",
        "                    \"answer\": sample[\"entity_value\"],\n",
        "                }\n",
        "            ]\n",
        "        }\n",
        "\n",
        "datasets = {\n",
        "    \"train\": AmazonDataset(train_df.loc[:int(0.6 * train_df.shape[0])]),\n",
        "    \"val\": AmazonDataset(train_df.loc[int(0.6 * train_df.shape[0]): ]),\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 222,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oBtf28oLtfPa",
        "outputId": "7fe57f77-029b-4b58-ffc5-943f7df03082"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "DEVICE = \"cuda\"\n",
        "DTYPE = torch.float32 if DEVICE == \"cpu\" else torch.bfloat16  # CPU doesn't support float16. Also, switch to bfloat16 for GPU\n",
        "MD_REVISION = \"2024-04-02\"\n",
        "use_4bit = False\n",
        "use_lora = True  # must be true if using 4_bit and training.\n",
        "set_other_trainable = True  # to set embed layers trainable (fully trainable, not LoRA)\n",
        "quantization_config = None\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"vikhyatk/moondream2\", revision=MD_REVISION)\n",
        "\n",
        "moondream = AutoModelForCausalLM.from_pretrained(\n",
        "    \"vikhyatk/moondream2\",\n",
        "    revision=MD_REVISION,  # Fixed 'D_REVISION' to 'MD_REVISION'\n",
        "    trust_remote_code=True,\n",
        "    attn_implementation=\"flash_attention_2\" if DEVICE == \"cuda\" else None,\n",
        "    torch_dtype=DTYPE,\n",
        "    device_map={\"\": DEVICE},\n",
        "    cache_dir='',\n",
        "    quantization_config=quantization_config\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 223,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1600x1600 at 0x7F791AA83430>, 'qa': [{'question': 'Extract the item_weight from the text and only use these units gram, kilogram, microgram, milligram, ounce, pound, ton', 'answer': '500.0 gram'}]}\n"
          ]
        }
      ],
      "source": [
        "print(datasets['train'][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 224,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Moondream(\n",
            "  (vision_encoder): VisionEncoder(\n",
            "    (encoder): EncoderWrapper(\n",
            "      (model): ModuleDict(\n",
            "        (visual): VisionTransformer(\n",
            "          (patch_embed): LinearPatchEmbedding(\n",
            "            (linear): Linear(in_features=588, out_features=1152, bias=True)\n",
            "          )\n",
            "          (blocks): Sequential(\n",
            "            (0): VitBlock(\n",
            "              (attn): Attention(\n",
            "                (qkv): Linear(in_features=1152, out_features=3456, bias=True)\n",
            "                (proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
            "              )\n",
            "              (mlp): MLP(\n",
            "                (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
            "                (act): GELU(approximate='tanh')\n",
            "                (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
            "              )\n",
            "              (norm1): LayerNorm((1152,), eps=1e-05, elementwise_affine=True)\n",
            "              (norm2): LayerNorm((1152,), eps=1e-05, elementwise_affine=True)\n",
            "            )\n",
            "            (1): VitBlock(\n",
            "              (attn): Attention(\n",
            "                (qkv): Linear(in_features=1152, out_features=3456, bias=True)\n",
            "                (proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
            "              )\n",
            "              (mlp): MLP(\n",
            "                (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
            "                (act): GELU(approximate='tanh')\n",
            "                (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
            "              )\n",
            "              (norm1): LayerNorm((1152,), eps=1e-05, elementwise_affine=True)\n",
            "              (norm2): LayerNorm((1152,), eps=1e-05, elementwise_affine=True)\n",
            "            )\n",
            "            (2): VitBlock(\n",
            "              (attn): Attention(\n",
            "                (qkv): Linear(in_features=1152, out_features=3456, bias=True)\n",
            "                (proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
            "              )\n",
            "              (mlp): MLP(\n",
            "                (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
            "                (act): GELU(approximate='tanh')\n",
            "                (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
            "              )\n",
            "              (norm1): LayerNorm((1152,), eps=1e-05, elementwise_affine=True)\n",
            "              (norm2): LayerNorm((1152,), eps=1e-05, elementwise_affine=True)\n",
            "            )\n",
            "            (3): VitBlock(\n",
            "              (attn): Attention(\n",
            "                (qkv): Linear(in_features=1152, out_features=3456, bias=True)\n",
            "                (proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
            "              )\n",
            "              (mlp): MLP(\n",
            "                (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
            "                (act): GELU(approximate='tanh')\n",
            "                (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
            "              )\n",
            "              (norm1): LayerNorm((1152,), eps=1e-05, elementwise_affine=True)\n",
            "              (norm2): LayerNorm((1152,), eps=1e-05, elementwise_affine=True)\n",
            "            )\n",
            "            (4): VitBlock(\n",
            "              (attn): Attention(\n",
            "                (qkv): Linear(in_features=1152, out_features=3456, bias=True)\n",
            "                (proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
            "              )\n",
            "              (mlp): MLP(\n",
            "                (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
            "                (act): GELU(approximate='tanh')\n",
            "                (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
            "              )\n",
            "              (norm1): LayerNorm((1152,), eps=1e-05, elementwise_affine=True)\n",
            "              (norm2): LayerNorm((1152,), eps=1e-05, elementwise_affine=True)\n",
            "            )\n",
            "            (5): VitBlock(\n",
            "              (attn): Attention(\n",
            "                (qkv): Linear(in_features=1152, out_features=3456, bias=True)\n",
            "                (proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
            "              )\n",
            "              (mlp): MLP(\n",
            "                (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
            "                (act): GELU(approximate='tanh')\n",
            "                (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
            "              )\n",
            "              (norm1): LayerNorm((1152,), eps=1e-05, elementwise_affine=True)\n",
            "              (norm2): LayerNorm((1152,), eps=1e-05, elementwise_affine=True)\n",
            "            )\n",
            "            (6): VitBlock(\n",
            "              (attn): Attention(\n",
            "                (qkv): Linear(in_features=1152, out_features=3456, bias=True)\n",
            "                (proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
            "              )\n",
            "              (mlp): MLP(\n",
            "                (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
            "                (act): GELU(approximate='tanh')\n",
            "                (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
            "              )\n",
            "              (norm1): LayerNorm((1152,), eps=1e-05, elementwise_affine=True)\n",
            "              (norm2): LayerNorm((1152,), eps=1e-05, elementwise_affine=True)\n",
            "            )\n",
            "            (7): VitBlock(\n",
            "              (attn): Attention(\n",
            "                (qkv): Linear(in_features=1152, out_features=3456, bias=True)\n",
            "                (proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
            "              )\n",
            "              (mlp): MLP(\n",
            "                (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
            "                (act): GELU(approximate='tanh')\n",
            "                (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
            "              )\n",
            "              (norm1): LayerNorm((1152,), eps=1e-05, elementwise_affine=True)\n",
            "              (norm2): LayerNorm((1152,), eps=1e-05, elementwise_affine=True)\n",
            "            )\n",
            "            (8): VitBlock(\n",
            "              (attn): Attention(\n",
            "                (qkv): Linear(in_features=1152, out_features=3456, bias=True)\n",
            "                (proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
            "              )\n",
            "              (mlp): MLP(\n",
            "                (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
            "                (act): GELU(approximate='tanh')\n",
            "                (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
            "              )\n",
            "              (norm1): LayerNorm((1152,), eps=1e-05, elementwise_affine=True)\n",
            "              (norm2): LayerNorm((1152,), eps=1e-05, elementwise_affine=True)\n",
            "            )\n",
            "            (9): VitBlock(\n",
            "              (attn): Attention(\n",
            "                (qkv): Linear(in_features=1152, out_features=3456, bias=True)\n",
            "                (proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
            "              )\n",
            "              (mlp): MLP(\n",
            "                (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
            "                (act): GELU(approximate='tanh')\n",
            "                (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
            "              )\n",
            "              (norm1): LayerNorm((1152,), eps=1e-05, elementwise_affine=True)\n",
            "              (norm2): LayerNorm((1152,), eps=1e-05, elementwise_affine=True)\n",
            "            )\n",
            "            (10): VitBlock(\n",
            "              (attn): Attention(\n",
            "                (qkv): Linear(in_features=1152, out_features=3456, bias=True)\n",
            "                (proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
            "              )\n",
            "              (mlp): MLP(\n",
            "                (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
            "                (act): GELU(approximate='tanh')\n",
            "                (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
            "              )\n",
            "              (norm1): LayerNorm((1152,), eps=1e-05, elementwise_affine=True)\n",
            "              (norm2): LayerNorm((1152,), eps=1e-05, elementwise_affine=True)\n",
            "            )\n",
            "            (11): VitBlock(\n",
            "              (attn): Attention(\n",
            "                (qkv): Linear(in_features=1152, out_features=3456, bias=True)\n",
            "                (proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
            "              )\n",
            "              (mlp): MLP(\n",
            "                (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
            "                (act): GELU(approximate='tanh')\n",
            "                (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
            "              )\n",
            "              (norm1): LayerNorm((1152,), eps=1e-05, elementwise_affine=True)\n",
            "              (norm2): LayerNorm((1152,), eps=1e-05, elementwise_affine=True)\n",
            "            )\n",
            "            (12): VitBlock(\n",
            "              (attn): Attention(\n",
            "                (qkv): Linear(in_features=1152, out_features=3456, bias=True)\n",
            "                (proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
            "              )\n",
            "              (mlp): MLP(\n",
            "                (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
            "                (act): GELU(approximate='tanh')\n",
            "                (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
            "              )\n",
            "              (norm1): LayerNorm((1152,), eps=1e-05, elementwise_affine=True)\n",
            "              (norm2): LayerNorm((1152,), eps=1e-05, elementwise_affine=True)\n",
            "            )\n",
            "            (13): VitBlock(\n",
            "              (attn): Attention(\n",
            "                (qkv): Linear(in_features=1152, out_features=3456, bias=True)\n",
            "                (proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
            "              )\n",
            "              (mlp): MLP(\n",
            "                (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
            "                (act): GELU(approximate='tanh')\n",
            "                (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
            "              )\n",
            "              (norm1): LayerNorm((1152,), eps=1e-05, elementwise_affine=True)\n",
            "              (norm2): LayerNorm((1152,), eps=1e-05, elementwise_affine=True)\n",
            "            )\n",
            "            (14): VitBlock(\n",
            "              (attn): Attention(\n",
            "                (qkv): Linear(in_features=1152, out_features=3456, bias=True)\n",
            "                (proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
            "              )\n",
            "              (mlp): MLP(\n",
            "                (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
            "                (act): GELU(approximate='tanh')\n",
            "                (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
            "              )\n",
            "              (norm1): LayerNorm((1152,), eps=1e-05, elementwise_affine=True)\n",
            "              (norm2): LayerNorm((1152,), eps=1e-05, elementwise_affine=True)\n",
            "            )\n",
            "            (15): VitBlock(\n",
            "              (attn): Attention(\n",
            "                (qkv): Linear(in_features=1152, out_features=3456, bias=True)\n",
            "                (proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
            "              )\n",
            "              (mlp): MLP(\n",
            "                (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
            "                (act): GELU(approximate='tanh')\n",
            "                (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
            "              )\n",
            "              (norm1): LayerNorm((1152,), eps=1e-05, elementwise_affine=True)\n",
            "              (norm2): LayerNorm((1152,), eps=1e-05, elementwise_affine=True)\n",
            "            )\n",
            "            (16): VitBlock(\n",
            "              (attn): Attention(\n",
            "                (qkv): Linear(in_features=1152, out_features=3456, bias=True)\n",
            "                (proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
            "              )\n",
            "              (mlp): MLP(\n",
            "                (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
            "                (act): GELU(approximate='tanh')\n",
            "                (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
            "              )\n",
            "              (norm1): LayerNorm((1152,), eps=1e-05, elementwise_affine=True)\n",
            "              (norm2): LayerNorm((1152,), eps=1e-05, elementwise_affine=True)\n",
            "            )\n",
            "            (17): VitBlock(\n",
            "              (attn): Attention(\n",
            "                (qkv): Linear(in_features=1152, out_features=3456, bias=True)\n",
            "                (proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
            "              )\n",
            "              (mlp): MLP(\n",
            "                (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
            "                (act): GELU(approximate='tanh')\n",
            "                (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
            "              )\n",
            "              (norm1): LayerNorm((1152,), eps=1e-05, elementwise_affine=True)\n",
            "              (norm2): LayerNorm((1152,), eps=1e-05, elementwise_affine=True)\n",
            "            )\n",
            "            (18): VitBlock(\n",
            "              (attn): Attention(\n",
            "                (qkv): Linear(in_features=1152, out_features=3456, bias=True)\n",
            "                (proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
            "              )\n",
            "              (mlp): MLP(\n",
            "                (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
            "                (act): GELU(approximate='tanh')\n",
            "                (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
            "              )\n",
            "              (norm1): LayerNorm((1152,), eps=1e-05, elementwise_affine=True)\n",
            "              (norm2): LayerNorm((1152,), eps=1e-05, elementwise_affine=True)\n",
            "            )\n",
            "            (19): VitBlock(\n",
            "              (attn): Attention(\n",
            "                (qkv): Linear(in_features=1152, out_features=3456, bias=True)\n",
            "                (proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
            "              )\n",
            "              (mlp): MLP(\n",
            "                (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
            "                (act): GELU(approximate='tanh')\n",
            "                (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
            "              )\n",
            "              (norm1): LayerNorm((1152,), eps=1e-05, elementwise_affine=True)\n",
            "              (norm2): LayerNorm((1152,), eps=1e-05, elementwise_affine=True)\n",
            "            )\n",
            "            (20): VitBlock(\n",
            "              (attn): Attention(\n",
            "                (qkv): Linear(in_features=1152, out_features=3456, bias=True)\n",
            "                (proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
            "              )\n",
            "              (mlp): MLP(\n",
            "                (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
            "                (act): GELU(approximate='tanh')\n",
            "                (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
            "              )\n",
            "              (norm1): LayerNorm((1152,), eps=1e-05, elementwise_affine=True)\n",
            "              (norm2): LayerNorm((1152,), eps=1e-05, elementwise_affine=True)\n",
            "            )\n",
            "            (21): VitBlock(\n",
            "              (attn): Attention(\n",
            "                (qkv): Linear(in_features=1152, out_features=3456, bias=True)\n",
            "                (proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
            "              )\n",
            "              (mlp): MLP(\n",
            "                (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
            "                (act): GELU(approximate='tanh')\n",
            "                (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
            "              )\n",
            "              (norm1): LayerNorm((1152,), eps=1e-05, elementwise_affine=True)\n",
            "              (norm2): LayerNorm((1152,), eps=1e-05, elementwise_affine=True)\n",
            "            )\n",
            "            (22): VitBlock(\n",
            "              (attn): Attention(\n",
            "                (qkv): Linear(in_features=1152, out_features=3456, bias=True)\n",
            "                (proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
            "              )\n",
            "              (mlp): MLP(\n",
            "                (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
            "                (act): GELU(approximate='tanh')\n",
            "                (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
            "              )\n",
            "              (norm1): LayerNorm((1152,), eps=1e-05, elementwise_affine=True)\n",
            "              (norm2): LayerNorm((1152,), eps=1e-05, elementwise_affine=True)\n",
            "            )\n",
            "            (23): VitBlock(\n",
            "              (attn): Attention(\n",
            "                (qkv): Linear(in_features=1152, out_features=3456, bias=True)\n",
            "                (proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
            "              )\n",
            "              (mlp): MLP(\n",
            "                (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
            "                (act): GELU(approximate='tanh')\n",
            "                (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
            "              )\n",
            "              (norm1): LayerNorm((1152,), eps=1e-05, elementwise_affine=True)\n",
            "              (norm2): LayerNorm((1152,), eps=1e-05, elementwise_affine=True)\n",
            "            )\n",
            "            (24): VitBlock(\n",
            "              (attn): Attention(\n",
            "                (qkv): Linear(in_features=1152, out_features=3456, bias=True)\n",
            "                (proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
            "              )\n",
            "              (mlp): MLP(\n",
            "                (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
            "                (act): GELU(approximate='tanh')\n",
            "                (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
            "              )\n",
            "              (norm1): LayerNorm((1152,), eps=1e-05, elementwise_affine=True)\n",
            "              (norm2): LayerNorm((1152,), eps=1e-05, elementwise_affine=True)\n",
            "            )\n",
            "            (25): VitBlock(\n",
            "              (attn): Attention(\n",
            "                (qkv): Linear(in_features=1152, out_features=3456, bias=True)\n",
            "                (proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
            "              )\n",
            "              (mlp): MLP(\n",
            "                (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
            "                (act): GELU(approximate='tanh')\n",
            "                (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
            "              )\n",
            "              (norm1): LayerNorm((1152,), eps=1e-05, elementwise_affine=True)\n",
            "              (norm2): LayerNorm((1152,), eps=1e-05, elementwise_affine=True)\n",
            "            )\n",
            "            (26): VitBlock(\n",
            "              (attn): Attention(\n",
            "                (qkv): Linear(in_features=1152, out_features=3456, bias=True)\n",
            "                (proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
            "              )\n",
            "              (mlp): MLP(\n",
            "                (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
            "                (act): GELU(approximate='tanh')\n",
            "                (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
            "              )\n",
            "              (norm1): LayerNorm((1152,), eps=1e-05, elementwise_affine=True)\n",
            "              (norm2): LayerNorm((1152,), eps=1e-05, elementwise_affine=True)\n",
            "            )\n",
            "          )\n",
            "          (norm): LayerNorm((1152,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (projection): VisionProjection(\n",
            "      (mlp): MLP(\n",
            "        (fc1): Linear(in_features=1152, out_features=8192, bias=True)\n",
            "        (act): GELU(approximate='tanh')\n",
            "        (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
            "      )\n",
            "    )\n",
            "    (preprocess): Compose(\n",
            "          Resize(size=[378, 378], interpolation=InterpolationMode.BICUBIC, antialias=True)\n",
            "          ToImage()\n",
            "          ToDtype(scale=True)\n",
            "          Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5], inplace=False)\n",
            "    )\n",
            "  )\n",
            "  (text_model): PhiForCausalLM(\n",
            "    (transformer): PhiModel(\n",
            "      (embd): Embedding(\n",
            "        (wte): Embedding(51200, 2048)\n",
            "      )\n",
            "      (embed_dropout): Dropout(p=0.0, inplace=False)\n",
            "      (h): ModuleList(\n",
            "        (0-23): 24 x PhiDecoderLayer(\n",
            "          (mixer): PhiFlashAttention2(\n",
            "            (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
            "            (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
            "            (rotary_emb): PhiRotaryEmbedding()\n",
            "          )\n",
            "          (mlp): PhiMLP(\n",
            "            (activation_fn): NewGELUActivation()\n",
            "            (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
            "            (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
            "          )\n",
            "          (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
            "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (lm_head): CausalLMHead(\n",
            "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
            "      (linear): Linear(in_features=2048, out_features=51200, bias=True)\n",
            "    )\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "print(moondream)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 225,
      "metadata": {
        "id": "ZWRZpgAEvxOC"
      },
      "outputs": [],
      "source": [
        "lora_alpha = 32\n",
        "lora_rank = 64\n",
        "\n",
        "if use_lora:\n",
        "    from peft import LoraConfig\n",
        "    lora_config = LoraConfig(\n",
        "        r = lora_rank,\n",
        "        lora_alpha = lora_alpha,\n",
        "        target_modules = [\n",
        "            'proj', 'fc1', 'fc2', 'Wqkv', 'out_proj'\n",
        "        ],\n",
        "        lora_dropout = 0.1,\n",
        "        bias = 'none',\n",
        "        task_type = \"CAUSAL_LM\"\n",
        "    ) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 226,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fnCOM9Wbw6m2",
        "outputId": "7a470bd4-be26-44df-9ad2-e44535ac8c35"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable params: 74,422,272 || all params: 1,931,904,880 || trainable%: 3.8523\n"
          ]
        }
      ],
      "source": [
        "if use_lora:\n",
        "    from peft import get_peft_model\n",
        "    moondream = get_peft_model(moondream, lora_config)\n",
        "    moondream.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 207,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V31AYyFcxX0a",
        "outputId": "7669aca3-89c9-4439-b0b2-0252e031d4d5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MoondreamConfig {\n",
            "  \"_name_or_path\": \"vikhyatk/moondream2\",\n",
            "  \"architectures\": [\n",
            "    \"Moondream\"\n",
            "  ],\n",
            "  \"auto_map\": {\n",
            "    \"AutoConfig\": \"vikhyatk/moondream2--configuration_moondream.MoondreamConfig\",\n",
            "    \"AutoModelForCausalLM\": \"vikhyatk/moondream2--moondream.Moondream\"\n",
            "  },\n",
            "  \"model_type\": \"moondream1\",\n",
            "  \"phi_config\": {\n",
            "    \"model_type\": \"phi\"\n",
            "  },\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.44.2\"\n",
            "}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(moondream.config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 190,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bCh1QhJF1z2l",
        "outputId": "87ddd30a-045e-4906-f45e-d78b3d07bce3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using an LR scaling for LoRA Adapters 4.0\n"
          ]
        }
      ],
      "source": [
        "EPOCHS = 1\n",
        "BATCH_SIZE = 1\n",
        "GRAD_ACCUM_STEPS = 1\n",
        "\n",
        "LR = 1.5e-5\n",
        "if use_lora:\n",
        "    LR_scaling = lora_alpha / (lora_rank ** 0.5)\n",
        "    print(\"Using an LR scaling for LoRA Adapters\", LR_scaling)\n",
        "\n",
        "eval_freq = 0.25\n",
        "USE_WANDB = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 227,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "_9cbzqCS3cT3",
        "outputId": "0532dfad-ffd5-4027-b48e-bc7f8920b697"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from bitsandbytes.optim import Adam8bit\n",
        "import math\n",
        "from einops import rearrange\n",
        "from tqdm import tqdm\n",
        "\n",
        "ANSWER_EOS = \"<|endoftext|>\"\n",
        "\n",
        "# Number of tokens used to represent each image.\n",
        "IMG_TOKENS = 729\n",
        "# eval_steps = total_steps * eval_freq\n",
        "\n",
        "def collate_fn(batch):\n",
        "    images = [sample['image'] for sample in batch]\n",
        "    images = torch.stack(moondream.vision_encoder.preprocess(images))\n",
        "    images = rearrange(images,\n",
        "                       \"b c (h p1) (w p2) -> b (h w) (c p1 p2)\", p1 = 14, p2 = 14)\n",
        "    print(images.shape)\n",
        "    labels_acc = []\n",
        "    tokens_acc = []\n",
        "\n",
        "    for sample in batch:\n",
        "        toks = [tokenizer.bos_token_id]\n",
        "        labs = [-100] * (IMG_TOKENS + 1)\n",
        "\n",
        "        for qa in sample['qa']:\n",
        "            q_t = tokenizer(\n",
        "                f\"\\n\\nQuestion: {qa['question']}\\n\\nAnswer:\",\n",
        "                add_special_tokens=False\n",
        "            ).input_ids\n",
        "            toks.extend(q_t)\n",
        "            labs.extend([-100] * len(q_t))\n",
        "\n",
        "            a_t = tokenizer(\n",
        "                f\" {qa['answer']}{ANSWER_EOS}\",\n",
        "                add_special_tokens=False\n",
        "            ).input_ids\n",
        "            toks.extend(a_t)\n",
        "            labs.extend(a_t)\n",
        "\n",
        "        tokens_acc.append(toks)\n",
        "        labels_acc.append(labs)\n",
        "\n",
        "    max_len = -1\n",
        "    for labels in labels_acc:\n",
        "        max_len = max(max_len, len(labels))\n",
        "\n",
        "    attn_mask_acc = []\n",
        "\n",
        "    for i in range(len(batch)):\n",
        "        len_i = len(labels_acc[i])\n",
        "        pad_i = max_len - len_i\n",
        "\n",
        "        labels_acc[i].extend([-100] * pad_i)\n",
        "        tokens_acc[i].extend([tokenizer.eos_token_id] * pad_i)\n",
        "        attn_mask_acc.append([1] * len_i + [0] * pad_i)\n",
        "\n",
        "    return (\n",
        "        images,\n",
        "        torch.stack([torch.tensor(t, dtype=torch.long) for t in tokens_acc]),\n",
        "        torch.stack([torch.tensor(l, dtype=torch.long) for l in labels_acc]),\n",
        "        torch.stack([torch.tensor(a, dtype=torch.bool) for a in attn_mask_acc]),\n",
        "    )\n",
        "\n",
        "def compute_loss(batch):\n",
        "    images, tokens, labels, attn_mask = batch\n",
        "\n",
        "    images = images.to(DEVICE).to(DTYPE)\n",
        "    tokens = tokens.to(DEVICE)\n",
        "    labels = labels.to(DEVICE)\n",
        "    attn_mask = attn_mask.to(DEVICE)\n",
        "\n",
        "    # print(type(images))\n",
        "\n",
        "    with torch.no_grad():\n",
        "        img_embs = moondream.vision_encoder.encoder(images)\n",
        "        img_embs = moondream.vision_encoder.projection(img_embs)\n",
        "        tok_embs = moondream.text_model.get_input_embeddings()(tokens)\n",
        "        inputs_embeds = torch.cat((tok_embs[:, 0:1, :], img_embs, tok_embs[:, 1:, :]), dim=1)\n",
        "\n",
        "    outputs = moondream.text_model(\n",
        "        inputs_embeds=inputs_embeds,\n",
        "        labels=labels,\n",
        "        attention_mask=attn_mask,\n",
        "    )\n",
        "\n",
        "    return outputs.loss\n",
        "\n",
        "def lr_schedule(step, max_steps):\n",
        "    x = step / max_steps\n",
        "    if x < 0.1:\n",
        "        return 0.1 * LR + 0.9 * LR * x / 0.1\n",
        "    else:\n",
        "        return 0.1 * LR + 0.9 * LR * (1 + math.cos(math.pi * (x - 0.1))) / 2\n",
        "\n",
        "dataloaders = {\n",
        "    \"train\": DataLoader(\n",
        "        datasets['train'],\n",
        "        batch_size=BATCH_SIZE,\n",
        "        shuffle=True,\n",
        "        collate_fn=collate_fn,\n",
        "    ),\n",
        "    'validation': DataLoader(\n",
        "        datasets['val'],\n",
        "        batch_size = BATCH_SIZE,\n",
        "        # shuffle = True,\n",
        "        collate_fn = collate_fn\n",
        "    )\n",
        "}\n",
        "\n",
        "moondream.text_model.train()\n",
        "# moondream.text_model.transformer.gradient_checkpointing_enable()\n",
        "\n",
        "total_steps = EPOCHS * len(dataloaders[\"train\"]) // GRAD_ACCUM_STEPS\n",
        "eval_steps = total_steps * eval_freq\n",
        "\n",
        "# lora parameters\n",
        "lora_params = []\n",
        "# lora_names = []\n",
        "for name, module in moondream.text_model.named_modules():\n",
        "    if 'lora' in name:\n",
        "        # lora_names.extend([n for n, p in module.named_parameters() if p.requires_grad])\n",
        "        lora_params.extend([p for p in module.parameters() if p.requires_grad])\n",
        "\n",
        "for name, param in moondream.named_parameters():\n",
        "  if('vision_encoder' in name):\n",
        "    param.requires_grad = False\n",
        "         "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 228,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/vishwa/Enter/lib/python3.9/site-packages/torch/_compile.py:24: UserWarning: optimizer contains a parameter group with duplicate parameters; in future, this will cause an error; see github.com/pytorch/pytorch/issues/40967 for more information\n",
            "  return torch._dynamo.disable(fn, recursive)(*args, **kwargs)\n",
            "Epoch 1/1:   0%|          | 0/158316 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 729, 588])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/1:   0%|          | 1/158316 [00:24<1078:06:24, 24.52s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 729, 588])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/1:   0%|          | 1/158316 [00:55<2423:38:03, 55.11s/it]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[228], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoss doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt require gradients\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     17\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss \u001b[38;5;241m/\u001b[39m GRAD_ACCUM_STEPS  \u001b[38;5;66;03m# Scale the loss\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (i) \u001b[38;5;241m%\u001b[39m GRAD_ACCUM_STEPS \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     21\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
            "File \u001b[0;32m~/Enter/lib/python3.9/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Enter/lib/python3.9/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "optimizer = Adam8bit(\n",
        "    [\n",
        "        {\"params\": lora_params},\n",
        "    ],\n",
        "    lr=LR * 0.1,\n",
        "    betas=(0.9, 0.95),\n",
        "    eps=1e-6\n",
        ")\n",
        "\n",
        "i = 0\n",
        "for epoch in range(EPOCHS):\n",
        "    for batch in tqdm(dataloaders[\"train\"], desc=f\"Epoch {epoch + 1}/{EPOCHS}\"):\n",
        "        i += 1\n",
        "        loss = compute_loss(batch)\n",
        "        if not loss.requires_grad:\n",
        "            raise ValueError(\"Loss doesn't require gradients\")\n",
        "        loss = loss / GRAD_ACCUM_STEPS  # Scale the loss\n",
        "        loss.backward()\n",
        "\n",
        "        if (i) % GRAD_ACCUM_STEPS == 0:\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        lr = lr_schedule((i + 1) / GRAD_ACCUM_STEPS, total_steps)\n",
        "        for param_group in optimizer.param_groups:\n",
        "            if param_group['lr'] == lora_params:\n",
        "                param_group['lr'] = lr * LR_scaling\n",
        "\n",
        "\n",
        "        if i % eval_steps == 0 and USE_WANDB:\n",
        "            val_loss = 0\n",
        "            for val_batch in tqdm(dataloaders['validation'], desc=\"Validation\"):\n",
        "                with torch.no_grad():\n",
        "                    val_loss += compute_loss(val_batch).item()\n",
        "            val_loss /= len(dataloaders['validation'])\n",
        "            wandb.log({\"loss/val\": val_loss})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 217,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "base_model.model.vision_encoder.encoder.model.visual.pos_embed: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.patch_embed.linear.weight: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.patch_embed.linear.bias: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.0.attn.qkv.weight: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.0.attn.qkv.bias: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.0.attn.proj.base_layer.weight: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.0.attn.proj.base_layer.bias: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.0.attn.proj.lora_A.default.weight: requires_grad = True\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.0.attn.proj.lora_B.default.weight: requires_grad = True\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.0.mlp.fc1.base_layer.weight: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.0.mlp.fc1.base_layer.bias: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.0.mlp.fc1.lora_A.default.weight: requires_grad = True\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.0.mlp.fc1.lora_B.default.weight: requires_grad = True\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.0.mlp.fc2.base_layer.weight: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.0.mlp.fc2.base_layer.bias: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.0.mlp.fc2.lora_A.default.weight: requires_grad = True\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.0.mlp.fc2.lora_B.default.weight: requires_grad = True\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.0.norm1.weight: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.0.norm1.bias: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.0.norm2.weight: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.0.norm2.bias: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.1.attn.qkv.weight: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.1.attn.qkv.bias: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.1.attn.proj.base_layer.weight: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.1.attn.proj.base_layer.bias: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.1.attn.proj.lora_A.default.weight: requires_grad = True\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.1.attn.proj.lora_B.default.weight: requires_grad = True\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.1.mlp.fc1.base_layer.weight: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.1.mlp.fc1.base_layer.bias: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.1.mlp.fc1.lora_A.default.weight: requires_grad = True\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.1.mlp.fc1.lora_B.default.weight: requires_grad = True\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.1.mlp.fc2.base_layer.weight: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.1.mlp.fc2.base_layer.bias: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.1.mlp.fc2.lora_A.default.weight: requires_grad = True\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.1.mlp.fc2.lora_B.default.weight: requires_grad = True\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.1.norm1.weight: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.1.norm1.bias: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.1.norm2.weight: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.1.norm2.bias: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.2.attn.qkv.weight: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.2.attn.qkv.bias: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.2.attn.proj.base_layer.weight: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.2.attn.proj.base_layer.bias: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.2.attn.proj.lora_A.default.weight: requires_grad = True\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.2.attn.proj.lora_B.default.weight: requires_grad = True\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.2.mlp.fc1.base_layer.weight: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.2.mlp.fc1.base_layer.bias: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.2.mlp.fc1.lora_A.default.weight: requires_grad = True\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.2.mlp.fc1.lora_B.default.weight: requires_grad = True\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.2.mlp.fc2.base_layer.weight: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.2.mlp.fc2.base_layer.bias: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.2.mlp.fc2.lora_A.default.weight: requires_grad = True\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.2.mlp.fc2.lora_B.default.weight: requires_grad = True\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.2.norm1.weight: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.2.norm1.bias: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.2.norm2.weight: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.2.norm2.bias: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.3.attn.qkv.weight: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.3.attn.qkv.bias: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.3.attn.proj.base_layer.weight: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.3.attn.proj.base_layer.bias: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.3.attn.proj.lora_A.default.weight: requires_grad = True\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.3.attn.proj.lora_B.default.weight: requires_grad = True\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.3.mlp.fc1.base_layer.weight: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.3.mlp.fc1.base_layer.bias: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.3.mlp.fc1.lora_A.default.weight: requires_grad = True\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.3.mlp.fc1.lora_B.default.weight: requires_grad = True\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.3.mlp.fc2.base_layer.weight: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.3.mlp.fc2.base_layer.bias: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.3.mlp.fc2.lora_A.default.weight: requires_grad = True\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.3.mlp.fc2.lora_B.default.weight: requires_grad = True\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.3.norm1.weight: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.3.norm1.bias: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.3.norm2.weight: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.3.norm2.bias: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.4.attn.qkv.weight: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.4.attn.qkv.bias: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.4.attn.proj.base_layer.weight: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.4.attn.proj.base_layer.bias: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.4.attn.proj.lora_A.default.weight: requires_grad = True\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.4.attn.proj.lora_B.default.weight: requires_grad = True\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.4.mlp.fc1.base_layer.weight: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.4.mlp.fc1.base_layer.bias: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.4.mlp.fc1.lora_A.default.weight: requires_grad = True\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.4.mlp.fc1.lora_B.default.weight: requires_grad = True\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.4.mlp.fc2.base_layer.weight: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.4.mlp.fc2.base_layer.bias: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.4.mlp.fc2.lora_A.default.weight: requires_grad = True\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.4.mlp.fc2.lora_B.default.weight: requires_grad = True\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.4.norm1.weight: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.4.norm1.bias: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.4.norm2.weight: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.4.norm2.bias: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.5.attn.qkv.weight: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.5.attn.qkv.bias: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.5.attn.proj.base_layer.weight: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.5.attn.proj.base_layer.bias: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.5.attn.proj.lora_A.default.weight: requires_grad = True\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.5.attn.proj.lora_B.default.weight: requires_grad = True\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.5.mlp.fc1.base_layer.weight: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.5.mlp.fc1.base_layer.bias: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.5.mlp.fc1.lora_A.default.weight: requires_grad = True\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.5.mlp.fc1.lora_B.default.weight: requires_grad = True\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.5.mlp.fc2.base_layer.weight: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.5.mlp.fc2.base_layer.bias: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.5.mlp.fc2.lora_A.default.weight: requires_grad = True\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.5.mlp.fc2.lora_B.default.weight: requires_grad = True\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.5.norm1.weight: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.5.norm1.bias: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.5.norm2.weight: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.5.norm2.bias: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.6.attn.qkv.weight: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.6.attn.qkv.bias: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.6.attn.proj.base_layer.weight: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.6.attn.proj.base_layer.bias: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.6.attn.proj.lora_A.default.weight: requires_grad = True\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.6.attn.proj.lora_B.default.weight: requires_grad = True\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.6.mlp.fc1.base_layer.weight: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.6.mlp.fc1.base_layer.bias: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.6.mlp.fc1.lora_A.default.weight: requires_grad = True\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.6.mlp.fc1.lora_B.default.weight: requires_grad = True\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.6.mlp.fc2.base_layer.weight: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.6.mlp.fc2.base_layer.bias: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.6.mlp.fc2.lora_A.default.weight: requires_grad = True\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.6.mlp.fc2.lora_B.default.weight: requires_grad = True\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.6.norm1.weight: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.6.norm1.bias: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.6.norm2.weight: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.6.norm2.bias: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.7.attn.qkv.weight: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.7.attn.qkv.bias: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.7.attn.proj.base_layer.weight: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.7.attn.proj.base_layer.bias: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.7.attn.proj.lora_A.default.weight: requires_grad = True\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.7.attn.proj.lora_B.default.weight: requires_grad = True\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.7.mlp.fc1.base_layer.weight: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.7.mlp.fc1.base_layer.bias: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.7.mlp.fc1.lora_A.default.weight: requires_grad = True\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.7.mlp.fc1.lora_B.default.weight: requires_grad = True\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.7.mlp.fc2.base_layer.weight: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.7.mlp.fc2.base_layer.bias: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.7.mlp.fc2.lora_A.default.weight: requires_grad = True\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.7.mlp.fc2.lora_B.default.weight: requires_grad = True\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.7.norm1.weight: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.7.norm1.bias: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.7.norm2.weight: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.7.norm2.bias: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.8.attn.qkv.weight: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.8.attn.qkv.bias: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.8.attn.proj.base_layer.weight: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.8.attn.proj.base_layer.bias: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.8.attn.proj.lora_A.default.weight: requires_grad = True\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.8.attn.proj.lora_B.default.weight: requires_grad = True\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.8.mlp.fc1.base_layer.weight: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.8.mlp.fc1.base_layer.bias: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.8.mlp.fc1.lora_A.default.weight: requires_grad = True\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.8.mlp.fc1.lora_B.default.weight: requires_grad = True\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.8.mlp.fc2.base_layer.weight: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.8.mlp.fc2.base_layer.bias: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.8.mlp.fc2.lora_A.default.weight: requires_grad = True\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.8.mlp.fc2.lora_B.default.weight: requires_grad = True\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.8.norm1.weight: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.8.norm1.bias: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.8.norm2.weight: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.8.norm2.bias: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.9.attn.qkv.weight: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.9.attn.qkv.bias: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.9.attn.proj.base_layer.weight: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.9.attn.proj.base_layer.bias: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.9.attn.proj.lora_A.default.weight: requires_grad = True\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.9.attn.proj.lora_B.default.weight: requires_grad = True\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.9.mlp.fc1.base_layer.weight: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.9.mlp.fc1.base_layer.bias: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.9.mlp.fc1.lora_A.default.weight: requires_grad = True\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.9.mlp.fc1.lora_B.default.weight: requires_grad = True\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.9.mlp.fc2.base_layer.weight: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.9.mlp.fc2.base_layer.bias: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.9.mlp.fc2.lora_A.default.weight: requires_grad = True\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.9.mlp.fc2.lora_B.default.weight: requires_grad = True\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.9.norm1.weight: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.9.norm1.bias: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.9.norm2.weight: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.9.norm2.bias: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.10.attn.qkv.weight: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.10.attn.qkv.bias: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.10.attn.proj.base_layer.weight: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.10.attn.proj.base_layer.bias: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.10.attn.proj.lora_A.default.weight: requires_grad = True\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.10.attn.proj.lora_B.default.weight: requires_grad = True\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.10.mlp.fc1.base_layer.weight: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.10.mlp.fc1.base_layer.bias: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.10.mlp.fc1.lora_A.default.weight: requires_grad = True\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.10.mlp.fc1.lora_B.default.weight: requires_grad = True\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.10.mlp.fc2.base_layer.weight: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.10.mlp.fc2.base_layer.bias: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.10.mlp.fc2.lora_A.default.weight: requires_grad = True\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.10.mlp.fc2.lora_B.default.weight: requires_grad = True\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.10.norm1.weight: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.10.norm1.bias: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.10.norm2.weight: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.10.norm2.bias: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.11.attn.qkv.weight: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.11.attn.qkv.bias: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.11.attn.proj.base_layer.weight: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.11.attn.proj.base_layer.bias: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.11.attn.proj.lora_A.default.weight: requires_grad = True\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.11.attn.proj.lora_B.default.weight: requires_grad = True\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.11.mlp.fc1.base_layer.weight: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.11.mlp.fc1.base_layer.bias: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.11.mlp.fc1.lora_A.default.weight: requires_grad = True\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.11.mlp.fc1.lora_B.default.weight: requires_grad = True\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.11.mlp.fc2.base_layer.weight: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.11.mlp.fc2.base_layer.bias: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.11.mlp.fc2.lora_A.default.weight: requires_grad = True\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.11.mlp.fc2.lora_B.default.weight: requires_grad = True\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.11.norm1.weight: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.11.norm1.bias: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.11.norm2.weight: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.11.norm2.bias: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.12.attn.qkv.weight: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.12.attn.qkv.bias: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.12.attn.proj.base_layer.weight: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.12.attn.proj.base_layer.bias: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.12.attn.proj.lora_A.default.weight: requires_grad = True\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.12.attn.proj.lora_B.default.weight: requires_grad = True\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.12.mlp.fc1.base_layer.weight: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.12.mlp.fc1.base_layer.bias: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.12.mlp.fc1.lora_A.default.weight: requires_grad = True\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.12.mlp.fc1.lora_B.default.weight: requires_grad = True\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.12.mlp.fc2.base_layer.weight: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.12.mlp.fc2.base_layer.bias: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.12.mlp.fc2.lora_A.default.weight: requires_grad = True\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.12.mlp.fc2.lora_B.default.weight: requires_grad = True\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.12.norm1.weight: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.12.norm1.bias: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.12.norm2.weight: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.12.norm2.bias: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.13.attn.qkv.weight: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.13.attn.qkv.bias: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.13.attn.proj.base_layer.weight: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.13.attn.proj.base_layer.bias: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.13.attn.proj.lora_A.default.weight: requires_grad = True\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.13.attn.proj.lora_B.default.weight: requires_grad = True\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.13.mlp.fc1.base_layer.weight: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.13.mlp.fc1.base_layer.bias: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.13.mlp.fc1.lora_A.default.weight: requires_grad = True\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.13.mlp.fc1.lora_B.default.weight: requires_grad = True\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.13.mlp.fc2.base_layer.weight: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.13.mlp.fc2.base_layer.bias: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.13.mlp.fc2.lora_A.default.weight: requires_grad = True\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.13.mlp.fc2.lora_B.default.weight: requires_grad = True\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.13.norm1.weight: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.13.norm1.bias: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.13.norm2.weight: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.13.norm2.bias: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.14.attn.qkv.weight: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.14.attn.qkv.bias: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.14.attn.proj.base_layer.weight: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.14.attn.proj.base_layer.bias: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.14.attn.proj.lora_A.default.weight: requires_grad = True\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.14.attn.proj.lora_B.default.weight: requires_grad = True\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.14.mlp.fc1.base_layer.weight: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.14.mlp.fc1.base_layer.bias: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.14.mlp.fc1.lora_A.default.weight: requires_grad = True\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.14.mlp.fc1.lora_B.default.weight: requires_grad = True\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.14.mlp.fc2.base_layer.weight: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.14.mlp.fc2.base_layer.bias: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.14.mlp.fc2.lora_A.default.weight: requires_grad = True\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.14.mlp.fc2.lora_B.default.weight: requires_grad = True\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.14.norm1.weight: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.14.norm1.bias: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.14.norm2.weight: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.14.norm2.bias: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.15.attn.qkv.weight: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.15.attn.qkv.bias: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.15.attn.proj.base_layer.weight: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.15.attn.proj.base_layer.bias: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.15.attn.proj.lora_A.default.weight: requires_grad = True\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.15.attn.proj.lora_B.default.weight: requires_grad = True\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.15.mlp.fc1.base_layer.weight: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.15.mlp.fc1.base_layer.bias: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.15.mlp.fc1.lora_A.default.weight: requires_grad = True\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.15.mlp.fc1.lora_B.default.weight: requires_grad = True\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.15.mlp.fc2.base_layer.weight: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.15.mlp.fc2.base_layer.bias: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.15.mlp.fc2.lora_A.default.weight: requires_grad = True\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.15.mlp.fc2.lora_B.default.weight: requires_grad = True\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.15.norm1.weight: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.15.norm1.bias: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.15.norm2.weight: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.15.norm2.bias: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.16.attn.qkv.weight: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.16.attn.qkv.bias: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.16.attn.proj.base_layer.weight: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.16.attn.proj.base_layer.bias: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.16.attn.proj.lora_A.default.weight: requires_grad = True\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.16.attn.proj.lora_B.default.weight: requires_grad = True\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.16.mlp.fc1.base_layer.weight: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.16.mlp.fc1.base_layer.bias: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.16.mlp.fc1.lora_A.default.weight: requires_grad = True\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.16.mlp.fc1.lora_B.default.weight: requires_grad = True\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.16.mlp.fc2.base_layer.weight: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.16.mlp.fc2.base_layer.bias: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.16.mlp.fc2.lora_A.default.weight: requires_grad = True\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.16.mlp.fc2.lora_B.default.weight: requires_grad = True\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.16.norm1.weight: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.16.norm1.bias: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.16.norm2.weight: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.16.norm2.bias: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.17.attn.qkv.weight: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.17.attn.qkv.bias: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.17.attn.proj.base_layer.weight: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.17.attn.proj.base_layer.bias: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.17.attn.proj.lora_A.default.weight: requires_grad = True\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.17.attn.proj.lora_B.default.weight: requires_grad = True\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.17.mlp.fc1.base_layer.weight: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.17.mlp.fc1.base_layer.bias: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.17.mlp.fc1.lora_A.default.weight: requires_grad = True\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.17.mlp.fc1.lora_B.default.weight: requires_grad = True\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.17.mlp.fc2.base_layer.weight: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.17.mlp.fc2.base_layer.bias: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.17.mlp.fc2.lora_A.default.weight: requires_grad = True\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.17.mlp.fc2.lora_B.default.weight: requires_grad = True\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.17.norm1.weight: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.17.norm1.bias: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.17.norm2.weight: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.17.norm2.bias: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.18.attn.qkv.weight: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.18.attn.qkv.bias: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.18.attn.proj.base_layer.weight: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.18.attn.proj.base_layer.bias: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.18.attn.proj.lora_A.default.weight: requires_grad = True\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.18.attn.proj.lora_B.default.weight: requires_grad = True\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.18.mlp.fc1.base_layer.weight: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.18.mlp.fc1.base_layer.bias: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.18.mlp.fc1.lora_A.default.weight: requires_grad = True\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.18.mlp.fc1.lora_B.default.weight: requires_grad = True\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.18.mlp.fc2.base_layer.weight: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.18.mlp.fc2.base_layer.bias: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.18.mlp.fc2.lora_A.default.weight: requires_grad = True\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.18.mlp.fc2.lora_B.default.weight: requires_grad = True\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.18.norm1.weight: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.18.norm1.bias: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.18.norm2.weight: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.18.norm2.bias: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.19.attn.qkv.weight: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.19.attn.qkv.bias: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.19.attn.proj.base_layer.weight: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.19.attn.proj.base_layer.bias: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.19.attn.proj.lora_A.default.weight: requires_grad = True\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.19.attn.proj.lora_B.default.weight: requires_grad = True\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.19.mlp.fc1.base_layer.weight: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.19.mlp.fc1.base_layer.bias: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.19.mlp.fc1.lora_A.default.weight: requires_grad = True\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.19.mlp.fc1.lora_B.default.weight: requires_grad = True\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.19.mlp.fc2.base_layer.weight: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.19.mlp.fc2.base_layer.bias: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.19.mlp.fc2.lora_A.default.weight: requires_grad = True\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.19.mlp.fc2.lora_B.default.weight: requires_grad = True\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.19.norm1.weight: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.19.norm1.bias: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.19.norm2.weight: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.19.norm2.bias: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.20.attn.qkv.weight: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.20.attn.qkv.bias: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.20.attn.proj.base_layer.weight: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.20.attn.proj.base_layer.bias: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.20.attn.proj.lora_A.default.weight: requires_grad = True\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.20.attn.proj.lora_B.default.weight: requires_grad = True\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.20.mlp.fc1.base_layer.weight: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.20.mlp.fc1.base_layer.bias: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.20.mlp.fc1.lora_A.default.weight: requires_grad = True\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.20.mlp.fc1.lora_B.default.weight: requires_grad = True\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.20.mlp.fc2.base_layer.weight: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.20.mlp.fc2.base_layer.bias: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.20.mlp.fc2.lora_A.default.weight: requires_grad = True\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.20.mlp.fc2.lora_B.default.weight: requires_grad = True\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.20.norm1.weight: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.20.norm1.bias: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.20.norm2.weight: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.20.norm2.bias: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.21.attn.qkv.weight: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.21.attn.qkv.bias: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.21.attn.proj.base_layer.weight: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.21.attn.proj.base_layer.bias: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.21.attn.proj.lora_A.default.weight: requires_grad = True\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.21.attn.proj.lora_B.default.weight: requires_grad = True\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.21.mlp.fc1.base_layer.weight: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.21.mlp.fc1.base_layer.bias: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.21.mlp.fc1.lora_A.default.weight: requires_grad = True\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.21.mlp.fc1.lora_B.default.weight: requires_grad = True\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.21.mlp.fc2.base_layer.weight: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.21.mlp.fc2.base_layer.bias: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.21.mlp.fc2.lora_A.default.weight: requires_grad = True\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.21.mlp.fc2.lora_B.default.weight: requires_grad = True\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.21.norm1.weight: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.21.norm1.bias: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.21.norm2.weight: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.21.norm2.bias: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.22.attn.qkv.weight: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.22.attn.qkv.bias: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.22.attn.proj.base_layer.weight: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.22.attn.proj.base_layer.bias: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.22.attn.proj.lora_A.default.weight: requires_grad = True\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.22.attn.proj.lora_B.default.weight: requires_grad = True\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.22.mlp.fc1.base_layer.weight: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.22.mlp.fc1.base_layer.bias: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.22.mlp.fc1.lora_A.default.weight: requires_grad = True\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.22.mlp.fc1.lora_B.default.weight: requires_grad = True\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.22.mlp.fc2.base_layer.weight: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.22.mlp.fc2.base_layer.bias: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.22.mlp.fc2.lora_A.default.weight: requires_grad = True\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.22.mlp.fc2.lora_B.default.weight: requires_grad = True\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.22.norm1.weight: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.22.norm1.bias: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.22.norm2.weight: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.22.norm2.bias: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.23.attn.qkv.weight: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.23.attn.qkv.bias: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.23.attn.proj.base_layer.weight: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.23.attn.proj.base_layer.bias: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.23.attn.proj.lora_A.default.weight: requires_grad = True\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.23.attn.proj.lora_B.default.weight: requires_grad = True\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.23.mlp.fc1.base_layer.weight: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.23.mlp.fc1.base_layer.bias: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.23.mlp.fc1.lora_A.default.weight: requires_grad = True\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.23.mlp.fc1.lora_B.default.weight: requires_grad = True\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.23.mlp.fc2.base_layer.weight: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.23.mlp.fc2.base_layer.bias: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.23.mlp.fc2.lora_A.default.weight: requires_grad = True\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.23.mlp.fc2.lora_B.default.weight: requires_grad = True\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.23.norm1.weight: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.23.norm1.bias: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.23.norm2.weight: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.23.norm2.bias: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.24.attn.qkv.weight: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.24.attn.qkv.bias: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.24.attn.proj.base_layer.weight: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.24.attn.proj.base_layer.bias: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.24.attn.proj.lora_A.default.weight: requires_grad = True\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.24.attn.proj.lora_B.default.weight: requires_grad = True\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.24.mlp.fc1.base_layer.weight: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.24.mlp.fc1.base_layer.bias: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.24.mlp.fc1.lora_A.default.weight: requires_grad = True\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.24.mlp.fc1.lora_B.default.weight: requires_grad = True\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.24.mlp.fc2.base_layer.weight: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.24.mlp.fc2.base_layer.bias: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.24.mlp.fc2.lora_A.default.weight: requires_grad = True\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.24.mlp.fc2.lora_B.default.weight: requires_grad = True\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.24.norm1.weight: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.24.norm1.bias: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.24.norm2.weight: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.24.norm2.bias: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.25.attn.qkv.weight: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.25.attn.qkv.bias: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.25.attn.proj.base_layer.weight: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.25.attn.proj.base_layer.bias: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.25.attn.proj.lora_A.default.weight: requires_grad = True\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.25.attn.proj.lora_B.default.weight: requires_grad = True\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.25.mlp.fc1.base_layer.weight: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.25.mlp.fc1.base_layer.bias: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.25.mlp.fc1.lora_A.default.weight: requires_grad = True\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.25.mlp.fc1.lora_B.default.weight: requires_grad = True\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.25.mlp.fc2.base_layer.weight: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.25.mlp.fc2.base_layer.bias: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.25.mlp.fc2.lora_A.default.weight: requires_grad = True\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.25.mlp.fc2.lora_B.default.weight: requires_grad = True\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.25.norm1.weight: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.25.norm1.bias: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.25.norm2.weight: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.25.norm2.bias: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.26.attn.qkv.weight: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.26.attn.qkv.bias: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.26.attn.proj.base_layer.weight: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.26.attn.proj.base_layer.bias: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.26.attn.proj.lora_A.default.weight: requires_grad = True\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.26.attn.proj.lora_B.default.weight: requires_grad = True\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.26.mlp.fc1.base_layer.weight: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.26.mlp.fc1.base_layer.bias: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.26.mlp.fc1.lora_A.default.weight: requires_grad = True\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.26.mlp.fc1.lora_B.default.weight: requires_grad = True\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.26.mlp.fc2.base_layer.weight: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.26.mlp.fc2.base_layer.bias: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.26.mlp.fc2.lora_A.default.weight: requires_grad = True\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.26.mlp.fc2.lora_B.default.weight: requires_grad = True\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.26.norm1.weight: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.26.norm1.bias: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.26.norm2.weight: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.blocks.26.norm2.bias: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.norm.weight: requires_grad = False\n",
            "base_model.model.vision_encoder.encoder.model.visual.norm.bias: requires_grad = False\n",
            "base_model.model.vision_encoder.projection.mlp.fc1.base_layer.weight: requires_grad = False\n",
            "base_model.model.vision_encoder.projection.mlp.fc1.base_layer.bias: requires_grad = False\n",
            "base_model.model.vision_encoder.projection.mlp.fc1.lora_A.default.weight: requires_grad = True\n",
            "base_model.model.vision_encoder.projection.mlp.fc1.lora_B.default.weight: requires_grad = True\n",
            "base_model.model.vision_encoder.projection.mlp.fc2.base_layer.weight: requires_grad = False\n",
            "base_model.model.vision_encoder.projection.mlp.fc2.base_layer.bias: requires_grad = False\n",
            "base_model.model.vision_encoder.projection.mlp.fc2.lora_A.default.weight: requires_grad = True\n",
            "base_model.model.vision_encoder.projection.mlp.fc2.lora_B.default.weight: requires_grad = True\n",
            "base_model.model.text_model.transformer.embd.wte.weight: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.0.mixer.Wqkv.base_layer.weight: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.0.mixer.Wqkv.base_layer.bias: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.0.mixer.Wqkv.lora_A.default.weight: requires_grad = True\n",
            "base_model.model.text_model.transformer.h.0.mixer.Wqkv.lora_B.default.weight: requires_grad = True\n",
            "base_model.model.text_model.transformer.h.0.mixer.out_proj.base_layer.weight: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.0.mixer.out_proj.base_layer.bias: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.0.mixer.out_proj.lora_A.default.weight: requires_grad = True\n",
            "base_model.model.text_model.transformer.h.0.mixer.out_proj.lora_B.default.weight: requires_grad = True\n",
            "base_model.model.text_model.transformer.h.0.mlp.fc1.base_layer.weight: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.0.mlp.fc1.base_layer.bias: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.0.mlp.fc1.lora_A.default.weight: requires_grad = True\n",
            "base_model.model.text_model.transformer.h.0.mlp.fc1.lora_B.default.weight: requires_grad = True\n",
            "base_model.model.text_model.transformer.h.0.mlp.fc2.base_layer.weight: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.0.mlp.fc2.base_layer.bias: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.0.mlp.fc2.lora_A.default.weight: requires_grad = True\n",
            "base_model.model.text_model.transformer.h.0.mlp.fc2.lora_B.default.weight: requires_grad = True\n",
            "base_model.model.text_model.transformer.h.0.ln.weight: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.0.ln.bias: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.1.mixer.Wqkv.base_layer.weight: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.1.mixer.Wqkv.base_layer.bias: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.1.mixer.Wqkv.lora_A.default.weight: requires_grad = True\n",
            "base_model.model.text_model.transformer.h.1.mixer.Wqkv.lora_B.default.weight: requires_grad = True\n",
            "base_model.model.text_model.transformer.h.1.mixer.out_proj.base_layer.weight: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.1.mixer.out_proj.base_layer.bias: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.1.mixer.out_proj.lora_A.default.weight: requires_grad = True\n",
            "base_model.model.text_model.transformer.h.1.mixer.out_proj.lora_B.default.weight: requires_grad = True\n",
            "base_model.model.text_model.transformer.h.1.mlp.fc1.base_layer.weight: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.1.mlp.fc1.base_layer.bias: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.1.mlp.fc1.lora_A.default.weight: requires_grad = True\n",
            "base_model.model.text_model.transformer.h.1.mlp.fc1.lora_B.default.weight: requires_grad = True\n",
            "base_model.model.text_model.transformer.h.1.mlp.fc2.base_layer.weight: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.1.mlp.fc2.base_layer.bias: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.1.mlp.fc2.lora_A.default.weight: requires_grad = True\n",
            "base_model.model.text_model.transformer.h.1.mlp.fc2.lora_B.default.weight: requires_grad = True\n",
            "base_model.model.text_model.transformer.h.1.ln.weight: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.1.ln.bias: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.2.mixer.Wqkv.base_layer.weight: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.2.mixer.Wqkv.base_layer.bias: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.2.mixer.Wqkv.lora_A.default.weight: requires_grad = True\n",
            "base_model.model.text_model.transformer.h.2.mixer.Wqkv.lora_B.default.weight: requires_grad = True\n",
            "base_model.model.text_model.transformer.h.2.mixer.out_proj.base_layer.weight: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.2.mixer.out_proj.base_layer.bias: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.2.mixer.out_proj.lora_A.default.weight: requires_grad = True\n",
            "base_model.model.text_model.transformer.h.2.mixer.out_proj.lora_B.default.weight: requires_grad = True\n",
            "base_model.model.text_model.transformer.h.2.mlp.fc1.base_layer.weight: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.2.mlp.fc1.base_layer.bias: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.2.mlp.fc1.lora_A.default.weight: requires_grad = True\n",
            "base_model.model.text_model.transformer.h.2.mlp.fc1.lora_B.default.weight: requires_grad = True\n",
            "base_model.model.text_model.transformer.h.2.mlp.fc2.base_layer.weight: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.2.mlp.fc2.base_layer.bias: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.2.mlp.fc2.lora_A.default.weight: requires_grad = True\n",
            "base_model.model.text_model.transformer.h.2.mlp.fc2.lora_B.default.weight: requires_grad = True\n",
            "base_model.model.text_model.transformer.h.2.ln.weight: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.2.ln.bias: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.3.mixer.Wqkv.base_layer.weight: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.3.mixer.Wqkv.base_layer.bias: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.3.mixer.Wqkv.lora_A.default.weight: requires_grad = True\n",
            "base_model.model.text_model.transformer.h.3.mixer.Wqkv.lora_B.default.weight: requires_grad = True\n",
            "base_model.model.text_model.transformer.h.3.mixer.out_proj.base_layer.weight: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.3.mixer.out_proj.base_layer.bias: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.3.mixer.out_proj.lora_A.default.weight: requires_grad = True\n",
            "base_model.model.text_model.transformer.h.3.mixer.out_proj.lora_B.default.weight: requires_grad = True\n",
            "base_model.model.text_model.transformer.h.3.mlp.fc1.base_layer.weight: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.3.mlp.fc1.base_layer.bias: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.3.mlp.fc1.lora_A.default.weight: requires_grad = True\n",
            "base_model.model.text_model.transformer.h.3.mlp.fc1.lora_B.default.weight: requires_grad = True\n",
            "base_model.model.text_model.transformer.h.3.mlp.fc2.base_layer.weight: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.3.mlp.fc2.base_layer.bias: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.3.mlp.fc2.lora_A.default.weight: requires_grad = True\n",
            "base_model.model.text_model.transformer.h.3.mlp.fc2.lora_B.default.weight: requires_grad = True\n",
            "base_model.model.text_model.transformer.h.3.ln.weight: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.3.ln.bias: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.4.mixer.Wqkv.base_layer.weight: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.4.mixer.Wqkv.base_layer.bias: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.4.mixer.Wqkv.lora_A.default.weight: requires_grad = True\n",
            "base_model.model.text_model.transformer.h.4.mixer.Wqkv.lora_B.default.weight: requires_grad = True\n",
            "base_model.model.text_model.transformer.h.4.mixer.out_proj.base_layer.weight: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.4.mixer.out_proj.base_layer.bias: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.4.mixer.out_proj.lora_A.default.weight: requires_grad = True\n",
            "base_model.model.text_model.transformer.h.4.mixer.out_proj.lora_B.default.weight: requires_grad = True\n",
            "base_model.model.text_model.transformer.h.4.mlp.fc1.base_layer.weight: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.4.mlp.fc1.base_layer.bias: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.4.mlp.fc1.lora_A.default.weight: requires_grad = True\n",
            "base_model.model.text_model.transformer.h.4.mlp.fc1.lora_B.default.weight: requires_grad = True\n",
            "base_model.model.text_model.transformer.h.4.mlp.fc2.base_layer.weight: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.4.mlp.fc2.base_layer.bias: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.4.mlp.fc2.lora_A.default.weight: requires_grad = True\n",
            "base_model.model.text_model.transformer.h.4.mlp.fc2.lora_B.default.weight: requires_grad = True\n",
            "base_model.model.text_model.transformer.h.4.ln.weight: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.4.ln.bias: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.5.mixer.Wqkv.base_layer.weight: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.5.mixer.Wqkv.base_layer.bias: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.5.mixer.Wqkv.lora_A.default.weight: requires_grad = True\n",
            "base_model.model.text_model.transformer.h.5.mixer.Wqkv.lora_B.default.weight: requires_grad = True\n",
            "base_model.model.text_model.transformer.h.5.mixer.out_proj.base_layer.weight: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.5.mixer.out_proj.base_layer.bias: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.5.mixer.out_proj.lora_A.default.weight: requires_grad = True\n",
            "base_model.model.text_model.transformer.h.5.mixer.out_proj.lora_B.default.weight: requires_grad = True\n",
            "base_model.model.text_model.transformer.h.5.mlp.fc1.base_layer.weight: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.5.mlp.fc1.base_layer.bias: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.5.mlp.fc1.lora_A.default.weight: requires_grad = True\n",
            "base_model.model.text_model.transformer.h.5.mlp.fc1.lora_B.default.weight: requires_grad = True\n",
            "base_model.model.text_model.transformer.h.5.mlp.fc2.base_layer.weight: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.5.mlp.fc2.base_layer.bias: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.5.mlp.fc2.lora_A.default.weight: requires_grad = True\n",
            "base_model.model.text_model.transformer.h.5.mlp.fc2.lora_B.default.weight: requires_grad = True\n",
            "base_model.model.text_model.transformer.h.5.ln.weight: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.5.ln.bias: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.6.mixer.Wqkv.base_layer.weight: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.6.mixer.Wqkv.base_layer.bias: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.6.mixer.Wqkv.lora_A.default.weight: requires_grad = True\n",
            "base_model.model.text_model.transformer.h.6.mixer.Wqkv.lora_B.default.weight: requires_grad = True\n",
            "base_model.model.text_model.transformer.h.6.mixer.out_proj.base_layer.weight: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.6.mixer.out_proj.base_layer.bias: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.6.mixer.out_proj.lora_A.default.weight: requires_grad = True\n",
            "base_model.model.text_model.transformer.h.6.mixer.out_proj.lora_B.default.weight: requires_grad = True\n",
            "base_model.model.text_model.transformer.h.6.mlp.fc1.base_layer.weight: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.6.mlp.fc1.base_layer.bias: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.6.mlp.fc1.lora_A.default.weight: requires_grad = True\n",
            "base_model.model.text_model.transformer.h.6.mlp.fc1.lora_B.default.weight: requires_grad = True\n",
            "base_model.model.text_model.transformer.h.6.mlp.fc2.base_layer.weight: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.6.mlp.fc2.base_layer.bias: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.6.mlp.fc2.lora_A.default.weight: requires_grad = True\n",
            "base_model.model.text_model.transformer.h.6.mlp.fc2.lora_B.default.weight: requires_grad = True\n",
            "base_model.model.text_model.transformer.h.6.ln.weight: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.6.ln.bias: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.7.mixer.Wqkv.base_layer.weight: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.7.mixer.Wqkv.base_layer.bias: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.7.mixer.Wqkv.lora_A.default.weight: requires_grad = True\n",
            "base_model.model.text_model.transformer.h.7.mixer.Wqkv.lora_B.default.weight: requires_grad = True\n",
            "base_model.model.text_model.transformer.h.7.mixer.out_proj.base_layer.weight: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.7.mixer.out_proj.base_layer.bias: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.7.mixer.out_proj.lora_A.default.weight: requires_grad = True\n",
            "base_model.model.text_model.transformer.h.7.mixer.out_proj.lora_B.default.weight: requires_grad = True\n",
            "base_model.model.text_model.transformer.h.7.mlp.fc1.base_layer.weight: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.7.mlp.fc1.base_layer.bias: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.7.mlp.fc1.lora_A.default.weight: requires_grad = True\n",
            "base_model.model.text_model.transformer.h.7.mlp.fc1.lora_B.default.weight: requires_grad = True\n",
            "base_model.model.text_model.transformer.h.7.mlp.fc2.base_layer.weight: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.7.mlp.fc2.base_layer.bias: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.7.mlp.fc2.lora_A.default.weight: requires_grad = True\n",
            "base_model.model.text_model.transformer.h.7.mlp.fc2.lora_B.default.weight: requires_grad = True\n",
            "base_model.model.text_model.transformer.h.7.ln.weight: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.7.ln.bias: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.8.mixer.Wqkv.base_layer.weight: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.8.mixer.Wqkv.base_layer.bias: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.8.mixer.Wqkv.lora_A.default.weight: requires_grad = True\n",
            "base_model.model.text_model.transformer.h.8.mixer.Wqkv.lora_B.default.weight: requires_grad = True\n",
            "base_model.model.text_model.transformer.h.8.mixer.out_proj.base_layer.weight: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.8.mixer.out_proj.base_layer.bias: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.8.mixer.out_proj.lora_A.default.weight: requires_grad = True\n",
            "base_model.model.text_model.transformer.h.8.mixer.out_proj.lora_B.default.weight: requires_grad = True\n",
            "base_model.model.text_model.transformer.h.8.mlp.fc1.base_layer.weight: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.8.mlp.fc1.base_layer.bias: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.8.mlp.fc1.lora_A.default.weight: requires_grad = True\n",
            "base_model.model.text_model.transformer.h.8.mlp.fc1.lora_B.default.weight: requires_grad = True\n",
            "base_model.model.text_model.transformer.h.8.mlp.fc2.base_layer.weight: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.8.mlp.fc2.base_layer.bias: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.8.mlp.fc2.lora_A.default.weight: requires_grad = True\n",
            "base_model.model.text_model.transformer.h.8.mlp.fc2.lora_B.default.weight: requires_grad = True\n",
            "base_model.model.text_model.transformer.h.8.ln.weight: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.8.ln.bias: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.9.mixer.Wqkv.base_layer.weight: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.9.mixer.Wqkv.base_layer.bias: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.9.mixer.Wqkv.lora_A.default.weight: requires_grad = True\n",
            "base_model.model.text_model.transformer.h.9.mixer.Wqkv.lora_B.default.weight: requires_grad = True\n",
            "base_model.model.text_model.transformer.h.9.mixer.out_proj.base_layer.weight: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.9.mixer.out_proj.base_layer.bias: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.9.mixer.out_proj.lora_A.default.weight: requires_grad = True\n",
            "base_model.model.text_model.transformer.h.9.mixer.out_proj.lora_B.default.weight: requires_grad = True\n",
            "base_model.model.text_model.transformer.h.9.mlp.fc1.base_layer.weight: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.9.mlp.fc1.base_layer.bias: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.9.mlp.fc1.lora_A.default.weight: requires_grad = True\n",
            "base_model.model.text_model.transformer.h.9.mlp.fc1.lora_B.default.weight: requires_grad = True\n",
            "base_model.model.text_model.transformer.h.9.mlp.fc2.base_layer.weight: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.9.mlp.fc2.base_layer.bias: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.9.mlp.fc2.lora_A.default.weight: requires_grad = True\n",
            "base_model.model.text_model.transformer.h.9.mlp.fc2.lora_B.default.weight: requires_grad = True\n",
            "base_model.model.text_model.transformer.h.9.ln.weight: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.9.ln.bias: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.10.mixer.Wqkv.base_layer.weight: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.10.mixer.Wqkv.base_layer.bias: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.10.mixer.Wqkv.lora_A.default.weight: requires_grad = True\n",
            "base_model.model.text_model.transformer.h.10.mixer.Wqkv.lora_B.default.weight: requires_grad = True\n",
            "base_model.model.text_model.transformer.h.10.mixer.out_proj.base_layer.weight: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.10.mixer.out_proj.base_layer.bias: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.10.mixer.out_proj.lora_A.default.weight: requires_grad = True\n",
            "base_model.model.text_model.transformer.h.10.mixer.out_proj.lora_B.default.weight: requires_grad = True\n",
            "base_model.model.text_model.transformer.h.10.mlp.fc1.base_layer.weight: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.10.mlp.fc1.base_layer.bias: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.10.mlp.fc1.lora_A.default.weight: requires_grad = True\n",
            "base_model.model.text_model.transformer.h.10.mlp.fc1.lora_B.default.weight: requires_grad = True\n",
            "base_model.model.text_model.transformer.h.10.mlp.fc2.base_layer.weight: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.10.mlp.fc2.base_layer.bias: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.10.mlp.fc2.lora_A.default.weight: requires_grad = True\n",
            "base_model.model.text_model.transformer.h.10.mlp.fc2.lora_B.default.weight: requires_grad = True\n",
            "base_model.model.text_model.transformer.h.10.ln.weight: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.10.ln.bias: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.11.mixer.Wqkv.base_layer.weight: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.11.mixer.Wqkv.base_layer.bias: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.11.mixer.Wqkv.lora_A.default.weight: requires_grad = True\n",
            "base_model.model.text_model.transformer.h.11.mixer.Wqkv.lora_B.default.weight: requires_grad = True\n",
            "base_model.model.text_model.transformer.h.11.mixer.out_proj.base_layer.weight: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.11.mixer.out_proj.base_layer.bias: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.11.mixer.out_proj.lora_A.default.weight: requires_grad = True\n",
            "base_model.model.text_model.transformer.h.11.mixer.out_proj.lora_B.default.weight: requires_grad = True\n",
            "base_model.model.text_model.transformer.h.11.mlp.fc1.base_layer.weight: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.11.mlp.fc1.base_layer.bias: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.11.mlp.fc1.lora_A.default.weight: requires_grad = True\n",
            "base_model.model.text_model.transformer.h.11.mlp.fc1.lora_B.default.weight: requires_grad = True\n",
            "base_model.model.text_model.transformer.h.11.mlp.fc2.base_layer.weight: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.11.mlp.fc2.base_layer.bias: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.11.mlp.fc2.lora_A.default.weight: requires_grad = True\n",
            "base_model.model.text_model.transformer.h.11.mlp.fc2.lora_B.default.weight: requires_grad = True\n",
            "base_model.model.text_model.transformer.h.11.ln.weight: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.11.ln.bias: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.12.mixer.Wqkv.base_layer.weight: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.12.mixer.Wqkv.base_layer.bias: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.12.mixer.Wqkv.lora_A.default.weight: requires_grad = True\n",
            "base_model.model.text_model.transformer.h.12.mixer.Wqkv.lora_B.default.weight: requires_grad = True\n",
            "base_model.model.text_model.transformer.h.12.mixer.out_proj.base_layer.weight: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.12.mixer.out_proj.base_layer.bias: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.12.mixer.out_proj.lora_A.default.weight: requires_grad = True\n",
            "base_model.model.text_model.transformer.h.12.mixer.out_proj.lora_B.default.weight: requires_grad = True\n",
            "base_model.model.text_model.transformer.h.12.mlp.fc1.base_layer.weight: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.12.mlp.fc1.base_layer.bias: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.12.mlp.fc1.lora_A.default.weight: requires_grad = True\n",
            "base_model.model.text_model.transformer.h.12.mlp.fc1.lora_B.default.weight: requires_grad = True\n",
            "base_model.model.text_model.transformer.h.12.mlp.fc2.base_layer.weight: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.12.mlp.fc2.base_layer.bias: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.12.mlp.fc2.lora_A.default.weight: requires_grad = True\n",
            "base_model.model.text_model.transformer.h.12.mlp.fc2.lora_B.default.weight: requires_grad = True\n",
            "base_model.model.text_model.transformer.h.12.ln.weight: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.12.ln.bias: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.13.mixer.Wqkv.base_layer.weight: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.13.mixer.Wqkv.base_layer.bias: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.13.mixer.Wqkv.lora_A.default.weight: requires_grad = True\n",
            "base_model.model.text_model.transformer.h.13.mixer.Wqkv.lora_B.default.weight: requires_grad = True\n",
            "base_model.model.text_model.transformer.h.13.mixer.out_proj.base_layer.weight: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.13.mixer.out_proj.base_layer.bias: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.13.mixer.out_proj.lora_A.default.weight: requires_grad = True\n",
            "base_model.model.text_model.transformer.h.13.mixer.out_proj.lora_B.default.weight: requires_grad = True\n",
            "base_model.model.text_model.transformer.h.13.mlp.fc1.base_layer.weight: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.13.mlp.fc1.base_layer.bias: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.13.mlp.fc1.lora_A.default.weight: requires_grad = True\n",
            "base_model.model.text_model.transformer.h.13.mlp.fc1.lora_B.default.weight: requires_grad = True\n",
            "base_model.model.text_model.transformer.h.13.mlp.fc2.base_layer.weight: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.13.mlp.fc2.base_layer.bias: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.13.mlp.fc2.lora_A.default.weight: requires_grad = True\n",
            "base_model.model.text_model.transformer.h.13.mlp.fc2.lora_B.default.weight: requires_grad = True\n",
            "base_model.model.text_model.transformer.h.13.ln.weight: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.13.ln.bias: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.14.mixer.Wqkv.base_layer.weight: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.14.mixer.Wqkv.base_layer.bias: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.14.mixer.Wqkv.lora_A.default.weight: requires_grad = True\n",
            "base_model.model.text_model.transformer.h.14.mixer.Wqkv.lora_B.default.weight: requires_grad = True\n",
            "base_model.model.text_model.transformer.h.14.mixer.out_proj.base_layer.weight: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.14.mixer.out_proj.base_layer.bias: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.14.mixer.out_proj.lora_A.default.weight: requires_grad = True\n",
            "base_model.model.text_model.transformer.h.14.mixer.out_proj.lora_B.default.weight: requires_grad = True\n",
            "base_model.model.text_model.transformer.h.14.mlp.fc1.base_layer.weight: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.14.mlp.fc1.base_layer.bias: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.14.mlp.fc1.lora_A.default.weight: requires_grad = True\n",
            "base_model.model.text_model.transformer.h.14.mlp.fc1.lora_B.default.weight: requires_grad = True\n",
            "base_model.model.text_model.transformer.h.14.mlp.fc2.base_layer.weight: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.14.mlp.fc2.base_layer.bias: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.14.mlp.fc2.lora_A.default.weight: requires_grad = True\n",
            "base_model.model.text_model.transformer.h.14.mlp.fc2.lora_B.default.weight: requires_grad = True\n",
            "base_model.model.text_model.transformer.h.14.ln.weight: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.14.ln.bias: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.15.mixer.Wqkv.base_layer.weight: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.15.mixer.Wqkv.base_layer.bias: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.15.mixer.Wqkv.lora_A.default.weight: requires_grad = True\n",
            "base_model.model.text_model.transformer.h.15.mixer.Wqkv.lora_B.default.weight: requires_grad = True\n",
            "base_model.model.text_model.transformer.h.15.mixer.out_proj.base_layer.weight: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.15.mixer.out_proj.base_layer.bias: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.15.mixer.out_proj.lora_A.default.weight: requires_grad = True\n",
            "base_model.model.text_model.transformer.h.15.mixer.out_proj.lora_B.default.weight: requires_grad = True\n",
            "base_model.model.text_model.transformer.h.15.mlp.fc1.base_layer.weight: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.15.mlp.fc1.base_layer.bias: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.15.mlp.fc1.lora_A.default.weight: requires_grad = True\n",
            "base_model.model.text_model.transformer.h.15.mlp.fc1.lora_B.default.weight: requires_grad = True\n",
            "base_model.model.text_model.transformer.h.15.mlp.fc2.base_layer.weight: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.15.mlp.fc2.base_layer.bias: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.15.mlp.fc2.lora_A.default.weight: requires_grad = True\n",
            "base_model.model.text_model.transformer.h.15.mlp.fc2.lora_B.default.weight: requires_grad = True\n",
            "base_model.model.text_model.transformer.h.15.ln.weight: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.15.ln.bias: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.16.mixer.Wqkv.base_layer.weight: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.16.mixer.Wqkv.base_layer.bias: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.16.mixer.Wqkv.lora_A.default.weight: requires_grad = True\n",
            "base_model.model.text_model.transformer.h.16.mixer.Wqkv.lora_B.default.weight: requires_grad = True\n",
            "base_model.model.text_model.transformer.h.16.mixer.out_proj.base_layer.weight: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.16.mixer.out_proj.base_layer.bias: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.16.mixer.out_proj.lora_A.default.weight: requires_grad = True\n",
            "base_model.model.text_model.transformer.h.16.mixer.out_proj.lora_B.default.weight: requires_grad = True\n",
            "base_model.model.text_model.transformer.h.16.mlp.fc1.base_layer.weight: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.16.mlp.fc1.base_layer.bias: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.16.mlp.fc1.lora_A.default.weight: requires_grad = True\n",
            "base_model.model.text_model.transformer.h.16.mlp.fc1.lora_B.default.weight: requires_grad = True\n",
            "base_model.model.text_model.transformer.h.16.mlp.fc2.base_layer.weight: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.16.mlp.fc2.base_layer.bias: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.16.mlp.fc2.lora_A.default.weight: requires_grad = True\n",
            "base_model.model.text_model.transformer.h.16.mlp.fc2.lora_B.default.weight: requires_grad = True\n",
            "base_model.model.text_model.transformer.h.16.ln.weight: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.16.ln.bias: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.17.mixer.Wqkv.base_layer.weight: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.17.mixer.Wqkv.base_layer.bias: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.17.mixer.Wqkv.lora_A.default.weight: requires_grad = True\n",
            "base_model.model.text_model.transformer.h.17.mixer.Wqkv.lora_B.default.weight: requires_grad = True\n",
            "base_model.model.text_model.transformer.h.17.mixer.out_proj.base_layer.weight: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.17.mixer.out_proj.base_layer.bias: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.17.mixer.out_proj.lora_A.default.weight: requires_grad = True\n",
            "base_model.model.text_model.transformer.h.17.mixer.out_proj.lora_B.default.weight: requires_grad = True\n",
            "base_model.model.text_model.transformer.h.17.mlp.fc1.base_layer.weight: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.17.mlp.fc1.base_layer.bias: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.17.mlp.fc1.lora_A.default.weight: requires_grad = True\n",
            "base_model.model.text_model.transformer.h.17.mlp.fc1.lora_B.default.weight: requires_grad = True\n",
            "base_model.model.text_model.transformer.h.17.mlp.fc2.base_layer.weight: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.17.mlp.fc2.base_layer.bias: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.17.mlp.fc2.lora_A.default.weight: requires_grad = True\n",
            "base_model.model.text_model.transformer.h.17.mlp.fc2.lora_B.default.weight: requires_grad = True\n",
            "base_model.model.text_model.transformer.h.17.ln.weight: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.17.ln.bias: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.18.mixer.Wqkv.base_layer.weight: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.18.mixer.Wqkv.base_layer.bias: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.18.mixer.Wqkv.lora_A.default.weight: requires_grad = True\n",
            "base_model.model.text_model.transformer.h.18.mixer.Wqkv.lora_B.default.weight: requires_grad = True\n",
            "base_model.model.text_model.transformer.h.18.mixer.out_proj.base_layer.weight: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.18.mixer.out_proj.base_layer.bias: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.18.mixer.out_proj.lora_A.default.weight: requires_grad = True\n",
            "base_model.model.text_model.transformer.h.18.mixer.out_proj.lora_B.default.weight: requires_grad = True\n",
            "base_model.model.text_model.transformer.h.18.mlp.fc1.base_layer.weight: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.18.mlp.fc1.base_layer.bias: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.18.mlp.fc1.lora_A.default.weight: requires_grad = True\n",
            "base_model.model.text_model.transformer.h.18.mlp.fc1.lora_B.default.weight: requires_grad = True\n",
            "base_model.model.text_model.transformer.h.18.mlp.fc2.base_layer.weight: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.18.mlp.fc2.base_layer.bias: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.18.mlp.fc2.lora_A.default.weight: requires_grad = True\n",
            "base_model.model.text_model.transformer.h.18.mlp.fc2.lora_B.default.weight: requires_grad = True\n",
            "base_model.model.text_model.transformer.h.18.ln.weight: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.18.ln.bias: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.19.mixer.Wqkv.base_layer.weight: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.19.mixer.Wqkv.base_layer.bias: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.19.mixer.Wqkv.lora_A.default.weight: requires_grad = True\n",
            "base_model.model.text_model.transformer.h.19.mixer.Wqkv.lora_B.default.weight: requires_grad = True\n",
            "base_model.model.text_model.transformer.h.19.mixer.out_proj.base_layer.weight: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.19.mixer.out_proj.base_layer.bias: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.19.mixer.out_proj.lora_A.default.weight: requires_grad = True\n",
            "base_model.model.text_model.transformer.h.19.mixer.out_proj.lora_B.default.weight: requires_grad = True\n",
            "base_model.model.text_model.transformer.h.19.mlp.fc1.base_layer.weight: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.19.mlp.fc1.base_layer.bias: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.19.mlp.fc1.lora_A.default.weight: requires_grad = True\n",
            "base_model.model.text_model.transformer.h.19.mlp.fc1.lora_B.default.weight: requires_grad = True\n",
            "base_model.model.text_model.transformer.h.19.mlp.fc2.base_layer.weight: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.19.mlp.fc2.base_layer.bias: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.19.mlp.fc2.lora_A.default.weight: requires_grad = True\n",
            "base_model.model.text_model.transformer.h.19.mlp.fc2.lora_B.default.weight: requires_grad = True\n",
            "base_model.model.text_model.transformer.h.19.ln.weight: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.19.ln.bias: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.20.mixer.Wqkv.base_layer.weight: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.20.mixer.Wqkv.base_layer.bias: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.20.mixer.Wqkv.lora_A.default.weight: requires_grad = True\n",
            "base_model.model.text_model.transformer.h.20.mixer.Wqkv.lora_B.default.weight: requires_grad = True\n",
            "base_model.model.text_model.transformer.h.20.mixer.out_proj.base_layer.weight: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.20.mixer.out_proj.base_layer.bias: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.20.mixer.out_proj.lora_A.default.weight: requires_grad = True\n",
            "base_model.model.text_model.transformer.h.20.mixer.out_proj.lora_B.default.weight: requires_grad = True\n",
            "base_model.model.text_model.transformer.h.20.mlp.fc1.base_layer.weight: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.20.mlp.fc1.base_layer.bias: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.20.mlp.fc1.lora_A.default.weight: requires_grad = True\n",
            "base_model.model.text_model.transformer.h.20.mlp.fc1.lora_B.default.weight: requires_grad = True\n",
            "base_model.model.text_model.transformer.h.20.mlp.fc2.base_layer.weight: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.20.mlp.fc2.base_layer.bias: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.20.mlp.fc2.lora_A.default.weight: requires_grad = True\n",
            "base_model.model.text_model.transformer.h.20.mlp.fc2.lora_B.default.weight: requires_grad = True\n",
            "base_model.model.text_model.transformer.h.20.ln.weight: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.20.ln.bias: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.21.mixer.Wqkv.base_layer.weight: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.21.mixer.Wqkv.base_layer.bias: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.21.mixer.Wqkv.lora_A.default.weight: requires_grad = True\n",
            "base_model.model.text_model.transformer.h.21.mixer.Wqkv.lora_B.default.weight: requires_grad = True\n",
            "base_model.model.text_model.transformer.h.21.mixer.out_proj.base_layer.weight: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.21.mixer.out_proj.base_layer.bias: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.21.mixer.out_proj.lora_A.default.weight: requires_grad = True\n",
            "base_model.model.text_model.transformer.h.21.mixer.out_proj.lora_B.default.weight: requires_grad = True\n",
            "base_model.model.text_model.transformer.h.21.mlp.fc1.base_layer.weight: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.21.mlp.fc1.base_layer.bias: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.21.mlp.fc1.lora_A.default.weight: requires_grad = True\n",
            "base_model.model.text_model.transformer.h.21.mlp.fc1.lora_B.default.weight: requires_grad = True\n",
            "base_model.model.text_model.transformer.h.21.mlp.fc2.base_layer.weight: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.21.mlp.fc2.base_layer.bias: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.21.mlp.fc2.lora_A.default.weight: requires_grad = True\n",
            "base_model.model.text_model.transformer.h.21.mlp.fc2.lora_B.default.weight: requires_grad = True\n",
            "base_model.model.text_model.transformer.h.21.ln.weight: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.21.ln.bias: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.22.mixer.Wqkv.base_layer.weight: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.22.mixer.Wqkv.base_layer.bias: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.22.mixer.Wqkv.lora_A.default.weight: requires_grad = True\n",
            "base_model.model.text_model.transformer.h.22.mixer.Wqkv.lora_B.default.weight: requires_grad = True\n",
            "base_model.model.text_model.transformer.h.22.mixer.out_proj.base_layer.weight: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.22.mixer.out_proj.base_layer.bias: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.22.mixer.out_proj.lora_A.default.weight: requires_grad = True\n",
            "base_model.model.text_model.transformer.h.22.mixer.out_proj.lora_B.default.weight: requires_grad = True\n",
            "base_model.model.text_model.transformer.h.22.mlp.fc1.base_layer.weight: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.22.mlp.fc1.base_layer.bias: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.22.mlp.fc1.lora_A.default.weight: requires_grad = True\n",
            "base_model.model.text_model.transformer.h.22.mlp.fc1.lora_B.default.weight: requires_grad = True\n",
            "base_model.model.text_model.transformer.h.22.mlp.fc2.base_layer.weight: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.22.mlp.fc2.base_layer.bias: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.22.mlp.fc2.lora_A.default.weight: requires_grad = True\n",
            "base_model.model.text_model.transformer.h.22.mlp.fc2.lora_B.default.weight: requires_grad = True\n",
            "base_model.model.text_model.transformer.h.22.ln.weight: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.22.ln.bias: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.23.mixer.Wqkv.base_layer.weight: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.23.mixer.Wqkv.base_layer.bias: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.23.mixer.Wqkv.lora_A.default.weight: requires_grad = True\n",
            "base_model.model.text_model.transformer.h.23.mixer.Wqkv.lora_B.default.weight: requires_grad = True\n",
            "base_model.model.text_model.transformer.h.23.mixer.out_proj.base_layer.weight: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.23.mixer.out_proj.base_layer.bias: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.23.mixer.out_proj.lora_A.default.weight: requires_grad = True\n",
            "base_model.model.text_model.transformer.h.23.mixer.out_proj.lora_B.default.weight: requires_grad = True\n",
            "base_model.model.text_model.transformer.h.23.mlp.fc1.base_layer.weight: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.23.mlp.fc1.base_layer.bias: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.23.mlp.fc1.lora_A.default.weight: requires_grad = True\n",
            "base_model.model.text_model.transformer.h.23.mlp.fc1.lora_B.default.weight: requires_grad = True\n",
            "base_model.model.text_model.transformer.h.23.mlp.fc2.base_layer.weight: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.23.mlp.fc2.base_layer.bias: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.23.mlp.fc2.lora_A.default.weight: requires_grad = True\n",
            "base_model.model.text_model.transformer.h.23.mlp.fc2.lora_B.default.weight: requires_grad = True\n",
            "base_model.model.text_model.transformer.h.23.ln.weight: requires_grad = False\n",
            "base_model.model.text_model.transformer.h.23.ln.bias: requires_grad = False\n",
            "base_model.model.text_model.lm_head.ln.weight: requires_grad = False\n",
            "base_model.model.text_model.lm_head.ln.bias: requires_grad = False\n",
            "base_model.model.text_model.lm_head.linear.weight: requires_grad = False\n",
            "base_model.model.text_model.lm_head.linear.bias: requires_grad = False\n"
          ]
        }
      ],
      "source": [
        "for name, param in model.named_parameters():\n",
        "    print(f\"{name}: requires_grad = {param.requires_grad}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
