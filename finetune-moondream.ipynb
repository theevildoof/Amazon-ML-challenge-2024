{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_l0DJa0kUauF",
        "outputId": "22b451c3-fe81-4fae-a0a8-c86bf6de9e0c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: flash-attn in /home/vishwa/Enter/lib/python3.9/site-packages (2.6.3)\n",
            "Requirement already satisfied: einops in /home/vishwa/Enter/lib/python3.9/site-packages (from flash-attn) (0.6.1)\n",
            "Requirement already satisfied: torch in /home/vishwa/Enter/lib/python3.9/site-packages (from flash-attn) (2.2.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/vishwa/Enter/lib/python3.9/site-packages (from torch->flash-attn) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/vishwa/Enter/lib/python3.9/site-packages (from torch->flash-attn) (8.9.2.26)\n",
            "Requirement already satisfied: sympy in /home/vishwa/Enter/lib/python3.9/site-packages (from torch->flash-attn) (1.12)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/vishwa/Enter/lib/python3.9/site-packages (from torch->flash-attn) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/vishwa/Enter/lib/python3.9/site-packages (from torch->flash-attn) (12.1.105)\n",
            "Requirement already satisfied: fsspec in /home/vishwa/Enter/lib/python3.9/site-packages (from torch->flash-attn) (2024.2.0)\n",
            "Requirement already satisfied: triton==2.2.0 in /home/vishwa/Enter/lib/python3.9/site-packages (from torch->flash-attn) (2.2.0)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/vishwa/Enter/lib/python3.9/site-packages (from torch->flash-attn) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/vishwa/Enter/lib/python3.9/site-packages (from torch->flash-attn) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /home/vishwa/Enter/lib/python3.9/site-packages (from torch->flash-attn) (2.19.3)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/vishwa/Enter/lib/python3.9/site-packages (from torch->flash-attn) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/vishwa/Enter/lib/python3.9/site-packages (from torch->flash-attn) (12.1.105)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/vishwa/Enter/lib/python3.9/site-packages (from torch->flash-attn) (12.1.105)\n",
            "Requirement already satisfied: networkx in /home/vishwa/Enter/lib/python3.9/site-packages (from torch->flash-attn) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /home/vishwa/Enter/lib/python3.9/site-packages (from torch->flash-attn) (3.1.3)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /home/vishwa/Enter/lib/python3.9/site-packages (from torch->flash-attn) (4.10.0)\n",
            "Requirement already satisfied: filelock in /home/vishwa/Enter/lib/python3.9/site-packages (from torch->flash-attn) (3.13.1)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/vishwa/Enter/lib/python3.9/site-packages (from torch->flash-attn) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/vishwa/Enter/lib/python3.9/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->flash-attn) (12.4.99)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /home/vishwa/Enter/lib/python3.9/site-packages (from jinja2->torch->flash-attn) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /home/vishwa/Enter/lib/python3.9/site-packages (from sympy->torch->flash-attn) (1.3.0)\n",
            "Collecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.43.3-py3-none-manylinux_2_24_x86_64.whl (137.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 137.5 MB 144 kB/s eta 0:00:01\n",
            "\u001b[?25hRequirement already satisfied: numpy in /home/vishwa/Enter/lib/python3.9/site-packages (from bitsandbytes) (1.26.4)\n",
            "Requirement already satisfied: torch in /home/vishwa/Enter/lib/python3.9/site-packages (from bitsandbytes) (2.2.1)\n",
            "Requirement already satisfied: networkx in /home/vishwa/Enter/lib/python3.9/site-packages (from torch->bitsandbytes) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /home/vishwa/Enter/lib/python3.9/site-packages (from torch->bitsandbytes) (3.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/vishwa/Enter/lib/python3.9/site-packages (from torch->bitsandbytes) (10.3.2.106)\n",
            "Requirement already satisfied: fsspec in /home/vishwa/Enter/lib/python3.9/site-packages (from torch->bitsandbytes) (2024.2.0)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/vishwa/Enter/lib/python3.9/site-packages (from torch->bitsandbytes) (12.1.0.106)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /home/vishwa/Enter/lib/python3.9/site-packages (from torch->bitsandbytes) (4.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/vishwa/Enter/lib/python3.9/site-packages (from torch->bitsandbytes) (12.1.105)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /home/vishwa/Enter/lib/python3.9/site-packages (from torch->bitsandbytes) (2.19.3)\n",
            "Requirement already satisfied: filelock in /home/vishwa/Enter/lib/python3.9/site-packages (from torch->bitsandbytes) (3.13.1)\n",
            "Requirement already satisfied: triton==2.2.0 in /home/vishwa/Enter/lib/python3.9/site-packages (from torch->bitsandbytes) (2.2.0)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/vishwa/Enter/lib/python3.9/site-packages (from torch->bitsandbytes) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/vishwa/Enter/lib/python3.9/site-packages (from torch->bitsandbytes) (12.1.105)\n",
            "Requirement already satisfied: sympy in /home/vishwa/Enter/lib/python3.9/site-packages (from torch->bitsandbytes) (1.12)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/vishwa/Enter/lib/python3.9/site-packages (from torch->bitsandbytes) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/vishwa/Enter/lib/python3.9/site-packages (from torch->bitsandbytes) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/vishwa/Enter/lib/python3.9/site-packages (from torch->bitsandbytes) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/vishwa/Enter/lib/python3.9/site-packages (from torch->bitsandbytes) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/vishwa/Enter/lib/python3.9/site-packages (from torch->bitsandbytes) (12.1.105)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/vishwa/Enter/lib/python3.9/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->bitsandbytes) (12.4.99)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /home/vishwa/Enter/lib/python3.9/site-packages (from jinja2->torch->bitsandbytes) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /home/vishwa/Enter/lib/python3.9/site-packages (from sympy->torch->bitsandbytes) (1.3.0)\n",
            "Installing collected packages: bitsandbytes\n",
            "Successfully installed bitsandbytes-0.43.3\n"
          ]
        }
      ],
      "source": [
        "!pip install flash-attn --no-build-isolation\n",
        "!pip install bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "_IgUpKBO30ad"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from PIL import Image\n",
        "import torch\n",
        "import os\n",
        "import pandas as pd\n",
        "import cv2\n",
        "import requests\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "CrA7Ot5oRWI8",
        "outputId": "7abf341d-cc64-47e6-a7bd-b76259806570"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image_link</th>\n",
              "      <th>group_id</th>\n",
              "      <th>entity_name</th>\n",
              "      <th>entity_value</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>https://m.media-amazon.com/images/I/61I9XdN6OF...</td>\n",
              "      <td>748919</td>\n",
              "      <td>item_weight</td>\n",
              "      <td>500.0 gram</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>https://m.media-amazon.com/images/I/71gSRbyXmo...</td>\n",
              "      <td>916768</td>\n",
              "      <td>item_volume</td>\n",
              "      <td>1.0 cup</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>https://m.media-amazon.com/images/I/61BZ4zrjZX...</td>\n",
              "      <td>459516</td>\n",
              "      <td>item_weight</td>\n",
              "      <td>0.709 gram</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>https://m.media-amazon.com/images/I/612mrlqiI4...</td>\n",
              "      <td>459516</td>\n",
              "      <td>item_weight</td>\n",
              "      <td>0.709 gram</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>https://m.media-amazon.com/images/I/617Tl40LOX...</td>\n",
              "      <td>731432</td>\n",
              "      <td>item_weight</td>\n",
              "      <td>1400 milligram</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                          image_link  group_id  entity_name  \\\n",
              "0  https://m.media-amazon.com/images/I/61I9XdN6OF...    748919  item_weight   \n",
              "1  https://m.media-amazon.com/images/I/71gSRbyXmo...    916768  item_volume   \n",
              "2  https://m.media-amazon.com/images/I/61BZ4zrjZX...    459516  item_weight   \n",
              "3  https://m.media-amazon.com/images/I/612mrlqiI4...    459516  item_weight   \n",
              "4  https://m.media-amazon.com/images/I/617Tl40LOX...    731432  item_weight   \n",
              "\n",
              "     entity_value  \n",
              "0      500.0 gram  \n",
              "1         1.0 cup  \n",
              "2      0.709 gram  \n",
              "3      0.709 gram  \n",
              "4  1400 milligram  "
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# loading dataset\n",
        "train_df = pd.read_csv(\"/home/vishwa/amzn-ml/student_resource 3/dataset/train.csv\")\n",
        "test_df = pd.read_csv('/home/vishwa/amzn-ml/student_resource 3/dataset/test.csv')\n",
        "train_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q-HwSYe2XgFE",
        "outputId": "183d501d-1f19-47a4-bee1-7bef9e5a1983"
      },
      "outputs": [],
      "source": [
        "entity_unit_map = {\n",
        "    'width': {'centimetre', 'foot', 'inch', 'metre', 'millimetre', 'yard'},\n",
        "    'depth': {'centimetre', 'foot', 'inch', 'metre', 'millimetre', 'yard'},\n",
        "    'height': {'centimetre', 'foot', 'inch', 'metre', 'millimetre', 'yard'},\n",
        "    'item_weight': {'gram',\n",
        "        'kilogram',\n",
        "        'microgram',\n",
        "        'milligram',\n",
        "        'ounce',\n",
        "        'pound',\n",
        "        'ton'},\n",
        "    'maximum_weight_recommendation': {'gram',\n",
        "        'kilogram',\n",
        "        'microgram',\n",
        "        'milligram',\n",
        "        'ounce',\n",
        "        'pound',\n",
        "        'ton'},\n",
        "    'voltage': {'kilovolt', 'millivolt', 'volt'},\n",
        "    'wattage': {'kilowatt', 'watt'},\n",
        "    'item_volume': {'centilitre',\n",
        "        'cubic foot',\n",
        "        'cubic inch',\n",
        "        'cup',\n",
        "        'decilitre',\n",
        "        'fluid ounce',\n",
        "        'gallon',\n",
        "        'imperial gallon',\n",
        "        'litre',\n",
        "        'microlitre',\n",
        "        'millilitre',\n",
        "        'pint',\n",
        "        'quart'}\n",
        "}\n",
        "\n",
        "allowed_units = {unit for entity in entity_unit_map for unit in entity_unit_map[entity]}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ED6hPk8dWAdF"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "# from datasets import load_dataset\n",
        "\n",
        "class AmazonDataset(Dataset):\n",
        "    def __init__(self, train_df):\n",
        "        self.data = train_df\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # print(idx)\n",
        "        sample = self.data.loc[idx, :]\n",
        "        # print(sample)\n",
        "        return {\n",
        "            \"image\": Image.open(requests.get(sample['image_link'], stream=True).raw), # Should be a PIL image\n",
        "            \"qa\": [\n",
        "                {\n",
        "                    \"question\": f\"Extract the {sample['entity_name']} from the text and only use these units {entity_unit_map[sample['entity_name']]}\",\n",
        "                    \"answer\": sample[\"entity_value\"],\n",
        "                }\n",
        "            ]\n",
        "        }\n",
        "\n",
        "datasets = {\n",
        "    \"train\": AmazonDataset(train_df),\n",
        "    \"test\": AmazonDataset(test_df),\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8A_Q9DP3ZYEs",
        "outputId": "0fa75349-c0f5-4021-ee96-5a1278285d9a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1600x1600>,\n",
              " 'qa': [{'question': \"Extract the item_weight from the text and only use these units {'ounce', 'kilogram', 'gram', 'microgram', 'ton', 'pound', 'milligram'}\",\n",
              "   'answer': '500.0 gram'}]}"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "next(iter(datasets['train']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "bClE2J1SRbPd"
      },
      "outputs": [],
      "source": [
        "# moon dream 2\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "DEVICE = \"cuda\"\n",
        "DTYPE = torch.float32 if DEVICE == \"cpu\" else torch.float16 # CPU doesn't support float16\n",
        "MD_REVISION = \"2024-07-23\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"vikhyatk/moondream2\", revision=MD_REVISION)\n",
        "moondream = AutoModelForCausalLM.from_pretrained(\n",
        "    \"vikhyatk/moondream2\", revision=MD_REVISION, trust_remote_code=True,\n",
        "    attn_implementation=\"flash_attention_2\" if DEVICE == \"cuda\" else None,\n",
        "    torch_dtype=DTYPE, device_map={\"\": DEVICE}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "9RKFQ0-MQrk_"
      },
      "outputs": [],
      "source": [
        "# Number of times to repeat the training dataset. Increasing this may cause the model to overfit or\n",
        "# lose generalization due to catastrophic forgetting. Decreasing it may cause the model to underfit.\n",
        "EPOCHS = 1\n",
        "\n",
        "# Number of samples to process in each batch. Set this to the highest value that doesn't cause an\n",
        "# out-of-memory error. Decrease it if you're running out of memory.\n",
        "BATCH_SIZE = 4\n",
        "\n",
        "# Number of batches to process before updating the model. You can use this to simulate a higher batch\n",
        "# size than your GPU can handle. Set this to 1 to disable gradient accumulation.\n",
        "GRAD_ACCUM_STEPS = 2\n",
        "\n",
        "# Learning rate for the Adam optimizer. Needs to be tuned on a case-by-case basis. As a general rule\n",
        "# of thumb, increase it by 1.4 times each time you double the effective batch size.\n",
        "#\n",
        "# Source: https://www.cs.princeton.edu/~smalladi/blog/2024/01/22/SDEs-ScalingRules/\n",
        "#\n",
        "# Note that we linearly warm the learning rate up from 0.1 * LR to LR over the first 10% of the\n",
        "# training run, and then decay it back to 0.1 * LR over the last 90% of the training run using a\n",
        "# cosine schedule.\n",
        "LR = 1e-5\n",
        "\n",
        "# Whether to use Weights and Biases for logging training metrics.\n",
        "USE_WANDB = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QGyEYaRTRnXT",
        "outputId": "777e061d-153e-4fee-a65b-9f7b2ccee86c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/1:   0%|          | 0/65965 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
            "Epoch 1/1:   0%|          | 10/65965 [04:08<645:49:54, 35.25s/it]"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from bitsandbytes.optim import Adam8bit\n",
        "import math\n",
        "from einops import rearrange\n",
        "from tqdm import tqdm\n",
        "\n",
        "ANSWER_EOS = \"<|endoftext|>\"\n",
        "\n",
        "# Number of tokens used to represent each image.\n",
        "IMG_TOKENS = 729\n",
        "\n",
        "def collate_fn(batch):\n",
        "    images = [sample['image'] for sample in batch]\n",
        "    images = [moondream.vision_encoder.preprocess(image) for image in images]\n",
        "\n",
        "    labels_acc = []\n",
        "    tokens_acc = []\n",
        "\n",
        "    for sample in batch:\n",
        "        toks = [tokenizer.bos_token_id]\n",
        "        labs = [-100] * (IMG_TOKENS + 1)\n",
        "\n",
        "        for qa in sample['qa']:\n",
        "            q_t = tokenizer(\n",
        "                f\"\\n\\nQuestion: {qa['question']}\\n\\nAnswer:\",\n",
        "                add_special_tokens=False\n",
        "            ).input_ids\n",
        "            toks.extend(q_t)\n",
        "            labs.extend([-100] * len(q_t))\n",
        "\n",
        "            a_t = tokenizer(\n",
        "                f\" {qa['answer']}{ANSWER_EOS}\",\n",
        "                add_special_tokens=False\n",
        "            ).input_ids\n",
        "            toks.extend(a_t)\n",
        "            labs.extend(a_t)\n",
        "\n",
        "        tokens_acc.append(toks)\n",
        "        labels_acc.append(labs)\n",
        "\n",
        "    max_len = -1\n",
        "    for labels in labels_acc:\n",
        "        max_len = max(max_len, len(labels))\n",
        "\n",
        "    attn_mask_acc = []\n",
        "\n",
        "    for i in range(len(batch)):\n",
        "        len_i = len(labels_acc[i])\n",
        "        pad_i = max_len - len_i\n",
        "\n",
        "        labels_acc[i].extend([-100] * pad_i)\n",
        "        tokens_acc[i].extend([tokenizer.eos_token_id] * pad_i)\n",
        "        attn_mask_acc.append([1] * len_i + [0] * pad_i)\n",
        "\n",
        "    return (\n",
        "        images,\n",
        "        torch.stack([torch.tensor(t, dtype=torch.long) for t in tokens_acc]),\n",
        "        torch.stack([torch.tensor(l, dtype=torch.long) for l in labels_acc]),\n",
        "        torch.stack([torch.tensor(a, dtype=torch.bool) for a in attn_mask_acc]),\n",
        "    )\n",
        "\n",
        "def compute_loss(batch):\n",
        "    images, tokens, labels, attn_mask = batch\n",
        "\n",
        "    tokens = tokens.to(DEVICE)\n",
        "    labels = labels.to(DEVICE)\n",
        "    attn_mask = attn_mask.to(DEVICE)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        img_embs = moondream.vision_encoder(images)\n",
        "\n",
        "    tok_embs = moondream.text_model.get_input_embeddings()(tokens)\n",
        "    inputs_embeds = torch.cat((tok_embs[:, 0:1, :], img_embs, tok_embs[:, 1:, :]), dim=1)\n",
        "\n",
        "    outputs = moondream.text_model(\n",
        "        inputs_embeds=inputs_embeds,\n",
        "        labels=labels,\n",
        "        attention_mask=attn_mask,\n",
        "    )\n",
        "\n",
        "    return outputs.loss\n",
        "\n",
        "def lr_schedule(step, max_steps):\n",
        "    x = step / max_steps\n",
        "    if x < 0.1:\n",
        "        return 0.1 * LR + 0.9 * LR * x / 0.1\n",
        "    else:\n",
        "        return 0.1 * LR + 0.9 * LR * (1 + math.cos(math.pi * (x - 0.1))) / 2\n",
        "\n",
        "dataloaders = {\n",
        "    \"train\": DataLoader(\n",
        "        datasets['train'],\n",
        "        batch_size=BATCH_SIZE,\n",
        "        shuffle=True,\n",
        "        collate_fn=collate_fn,\n",
        "    )\n",
        "}\n",
        "\n",
        "moondream.text_model.train()\n",
        "moondream.text_model.transformer.gradient_checkpointing_enable()\n",
        "\n",
        "total_steps = EPOCHS * len(dataloaders[\"train\"]) // GRAD_ACCUM_STEPS\n",
        "optimizer = Adam8bit(\n",
        "    [\n",
        "        {\"params\": moondream.text_model.parameters()},\n",
        "    ],\n",
        "    lr=LR * 0.1,\n",
        "    betas=(0.9, 0.95),\n",
        "    eps=1e-6\n",
        ")\n",
        "\n",
        "if USE_WANDB:\n",
        "    import wandb\n",
        "    wandb.init(\n",
        "        project=\"moondream-ft\",\n",
        "        config={\n",
        "            \"EPOCHS\": EPOCHS,\n",
        "            \"BATCH_SIZE\": BATCH_SIZE,\n",
        "            \"GRAD_ACCUM_STEPS\": GRAD_ACCUM_STEPS,\n",
        "            \"LR\": LR,\n",
        "        }\n",
        "    )\n",
        "\n",
        "i = 0\n",
        "for epoch in range(EPOCHS):\n",
        "    for batch in tqdm(dataloaders[\"train\"], desc=f\"Epoch {epoch + 1}/{EPOCHS}\"):\n",
        "        i += 1\n",
        "\n",
        "        loss = compute_loss(batch)\n",
        "        loss.backward()\n",
        "\n",
        "        if i % GRAD_ACCUM_STEPS == 0:\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            lr = lr_schedule(i / GRAD_ACCUM_STEPS, total_steps)\n",
        "            for param_group in optimizer.param_groups:\n",
        "                param_group['lr'] = lr\n",
        "\n",
        "        if USE_WANDB:\n",
        "            wandb.log({\n",
        "                \"loss/train\": loss.item(),\n",
        "                \"lr\": optimizer.param_groups[0]['lr']\n",
        "            })\n",
        "\n",
        "if USE_WANDB:\n",
        "    wandb.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "iirbNRHPY8b3",
        "outputId": "1ef37dbe-aabb-4386-9e17-3f185afd07d4"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'https://m.media-amazon.com/images/I/61e8xP3dqLL.jpg'"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_df.loc[57504]['image_link']"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
