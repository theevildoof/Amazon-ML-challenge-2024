{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: flash_attn in /home/vishwa/Enter/lib/python3.9/site-packages (2.6.3)\n",
      "Requirement already satisfied: torch in /home/vishwa/Enter/lib/python3.9/site-packages (from flash_attn) (2.2.1)\n",
      "Requirement already satisfied: einops in /home/vishwa/Enter/lib/python3.9/site-packages (from flash_attn) (0.6.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/vishwa/Enter/lib/python3.9/site-packages (from torch->flash_attn) (4.10.0)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /home/vishwa/Enter/lib/python3.9/site-packages (from torch->flash_attn) (2.19.3)\n",
      "Requirement already satisfied: filelock in /home/vishwa/Enter/lib/python3.9/site-packages (from torch->flash_attn) (3.13.1)\n",
      "Requirement already satisfied: fsspec in /home/vishwa/Enter/lib/python3.9/site-packages (from torch->flash_attn) (2024.2.0)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/vishwa/Enter/lib/python3.9/site-packages (from torch->flash_attn) (8.9.2.26)\n",
      "Requirement already satisfied: jinja2 in /home/vishwa/Enter/lib/python3.9/site-packages (from torch->flash_attn) (3.1.3)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/vishwa/Enter/lib/python3.9/site-packages (from torch->flash_attn) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/vishwa/Enter/lib/python3.9/site-packages (from torch->flash_attn) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/vishwa/Enter/lib/python3.9/site-packages (from torch->flash_attn) (12.1.105)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/vishwa/Enter/lib/python3.9/site-packages (from torch->flash_attn) (12.1.105)\n",
      "Requirement already satisfied: networkx in /home/vishwa/Enter/lib/python3.9/site-packages (from torch->flash_attn) (3.2.1)\n",
      "Requirement already satisfied: sympy in /home/vishwa/Enter/lib/python3.9/site-packages (from torch->flash_attn) (1.12)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/vishwa/Enter/lib/python3.9/site-packages (from torch->flash_attn) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/vishwa/Enter/lib/python3.9/site-packages (from torch->flash_attn) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/vishwa/Enter/lib/python3.9/site-packages (from torch->flash_attn) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/vishwa/Enter/lib/python3.9/site-packages (from torch->flash_attn) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/vishwa/Enter/lib/python3.9/site-packages (from torch->flash_attn) (11.4.5.107)\n",
      "Requirement already satisfied: triton==2.2.0 in /home/vishwa/Enter/lib/python3.9/site-packages (from torch->flash_attn) (2.2.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/vishwa/Enter/lib/python3.9/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->flash_attn) (12.4.99)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/vishwa/Enter/lib/python3.9/site-packages (from jinja2->torch->flash_attn) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/vishwa/Enter/lib/python3.9/site-packages (from sympy->torch->flash_attn) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vishwa/Enter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<OCR_WITH_REGION>': {'quad_boxes': [[759.2000122070312, 410.3999938964844, 914.4000244140625, 410.3999938964844, 914.4000244140625, 461.6000061035156, 759.2000122070312, 461.6000061035156], [196.0, 765.6000366210938, 348.0, 765.6000366210938, 348.0, 820.0, 196.0, 820.0]], 'labels': ['</s>20CM', '15CM']}}\n"
     ]
    }
   ],
   "source": [
    "%pip install flash_attn\n",
    "import requests\n",
    "\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import AutoProcessor, AutoModelForCausalLM \n",
    "\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"microsoft/Florence-2-large\", torch_dtype=torch_dtype, trust_remote_code=True).to(device)\n",
    "processor = AutoProcessor.from_pretrained(\"microsoft/Florence-2-large\", trust_remote_code=True)\n",
    "\n",
    "prompt = \"<OCR_WITH_REGION>\"\n",
    "\n",
    "\n",
    "image = Image.open('/home/vishwa/amzn-ml/student_resource 3/images/41-NCxNuBxL.jpg')\n",
    "\n",
    "inputs = processor(text=prompt, images=image, return_tensors=\"pt\").to(device, torch_dtype)\n",
    "\n",
    "generated_ids = model.generate(\n",
    "    input_ids=inputs[\"input_ids\"],\n",
    "    pixel_values=inputs[\"pixel_values\"],\n",
    "    max_new_tokens=1024,\n",
    "    num_beams=3,\n",
    "    do_sample=False\n",
    ")\n",
    "generated_text = processor.batch_decode(generated_ids, skip_special_tokens=False)[0]\n",
    "\n",
    "parsed_answer = processor.post_process_generation(generated_text, task=\"<OCR_WITH_REGION>\", image_size=(image.width, image.height))\n",
    "\n",
    "print(parsed_answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
