{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install timm flash_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import AutoProcessor, AutoModelForCausalLM \n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "from transformers import (AdamW, AutoProcessor, get_scheduler)\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0047609806060791016,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "config.json",
       "rate": null,
       "total": 2430,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d90318befca4ed38c144a042de9c6c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/2.43k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/microsoft/Florence-2-base-ft:\n",
      "- modeling_florence2.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.005196094512939453,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "processing_florence2.py",
       "rate": null,
       "total": 46372,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "479a88fc092e4ecba0341c02449e91ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "processing_florence2.py:   0%|          | 0.00/46.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/microsoft/Florence-2-base-ft:\n",
      "- processing_florence2.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "/home/vishwa/Enter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"microsoft/Florence-2-base-ft\", trust_remote_code=True).to(device)\n",
    "processor = AutoProcessor.from_pretrained(\"microsoft/Florence-2-base-ft\", trust_remote_code=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_example(task_prompt, text_prompt, image):\n",
    "    # prompt = task_prompt + text_input\n",
    "    image = Image.open(image)\n",
    "    inputs = processor(text=text_prompt, images=image, return_tensors=\"pt\").to(device, torch_dtype)\n",
    "    generated_ids = model.generate(\n",
    "        input_ids=inputs[\"input_ids\"],\n",
    "        pixel_values=inputs[\"pixel_values\"],\n",
    "        max_new_tokens=1024,\n",
    "        num_beams=3\n",
    "    )\n",
    "    generated_text = processor.batch_decode(generated_ids, skip_special_tokens=False)[0]\n",
    "    parsed_answer = processor.post_process_generation(generated_text, task=task_prompt, image_size=(image.width, image.height))\n",
    "    return parsed_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FinetuneDataset(Dataset):\n",
    "    def __init__(self, data, mode):\n",
    "        self.data = data\n",
    "        self.mode = mode\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        example = self.data.loc[idx]\n",
    "        # print(example)\n",
    "        task_prompt = \"<MORE_DETAILED_CAPTION>\" \n",
    "        text_prompt = example['entity_name']\n",
    "        answer = example['entity_value']\n",
    "        image_path = '/home/vishwa/amzn-ml/student_resource 3/train_images/' + example['image_link']\n",
    "        image = Image.open(image_path)\n",
    "        # image = Image.open(image)\n",
    "        # if image.mode != \"RGB\":\n",
    "            # image = image.convert(\"RGB\")\n",
    "        return task_prompt, text_prompt, answer, image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# task_prompt = \"<MORE_DETAILED_CAPTION>\" \n",
    "# text_prompt = \"numbers\"\n",
    "# # url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/car.jpg?download=true\"\n",
    "# image = Image.open('/home/vishwa/amzn-ml/student_resource 3/images/41uwo4PVnuL.jpg')\n",
    "# inputs = processor(text=text_prompt, images=image, return_tensors=\"pt\").to(device, torch_dtype)\n",
    "\n",
    "# generated_ids = model.generate(\n",
    "#     input_ids=inputs[\"input_ids\"],\n",
    "#     pixel_values=inputs[\"pixel_values\"],\n",
    "#     max_new_tokens=1024,\n",
    "#     num_beams=3,\n",
    "#     do_sample=False\n",
    "# )\n",
    "\n",
    "# generated_text = processor.batch_decode(generated_ids, skip_special_tokens=False)[0]\n",
    "\n",
    "# parsed_answer = processor.post_process_generation(generated_text, task=task_prompt, image_size=(image.width, image.height))\n",
    "\n",
    "# print(parsed_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Figuring out the valid & train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('student_resource 3/dataset/train.csv')\n",
    "test_df = pd.read_csv('student_resource 3/dataset/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train group size: 750\n",
      "Test group size: 924\n",
      "Overlap size: 638\n"
     ]
    }
   ],
   "source": [
    "train_groups = set(train_df['group_id'].unique())\n",
    "print('Train group size:', len(train_groups))\n",
    "test_groups = set(test_df['group_id'].unique())\n",
    "print('Test group size:', len(test_groups))\n",
    "overlap = list(train_groups.intersection(test_groups))\n",
    "print('Overlap size:', len(overlap))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "112"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_groups - set(overlap))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Since the test set has some groups that are not in the train set, my approach is to work "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split():\n",
    "    return  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['train/valid'] = np.where(train_df['group_id'].isin(overlap), 'train', 'valid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['image_link'] = train_df['image_link'].str.split('/').str[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dat = train_df[train_df['train/valid'] == 'train']\n",
    "valid_dat = train_df[train_df['train/valid'] == 'valid']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>image_link</th>\n",
       "      <th>group_id</th>\n",
       "      <th>entity_name</th>\n",
       "      <th>entity_value</th>\n",
       "      <th>train/valid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>61I9XdN6OFL.jpg</td>\n",
       "      <td>748919</td>\n",
       "      <td>item_weight</td>\n",
       "      <td>500.0 gram</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>71gSRbyXmoL.jpg</td>\n",
       "      <td>916768</td>\n",
       "      <td>item_volume</td>\n",
       "      <td>1.0 cup</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>61BZ4zrjZXL.jpg</td>\n",
       "      <td>459516</td>\n",
       "      <td>item_weight</td>\n",
       "      <td>0.709 gram</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>612mrlqiI4L.jpg</td>\n",
       "      <td>459516</td>\n",
       "      <td>item_weight</td>\n",
       "      <td>0.709 gram</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>617Tl40LOXL.jpg</td>\n",
       "      <td>731432</td>\n",
       "      <td>item_weight</td>\n",
       "      <td>1400 milligram</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260054</th>\n",
       "      <td>263854</td>\n",
       "      <td>612J1R1xHlL.jpg</td>\n",
       "      <td>558806</td>\n",
       "      <td>height</td>\n",
       "      <td>5.0 centimetre</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260055</th>\n",
       "      <td>263855</td>\n",
       "      <td>61Blzh2+28L.jpg</td>\n",
       "      <td>470067</td>\n",
       "      <td>height</td>\n",
       "      <td>8.5 inch</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260056</th>\n",
       "      <td>263856</td>\n",
       "      <td>51MsegDL9VL.jpg</td>\n",
       "      <td>204245</td>\n",
       "      <td>height</td>\n",
       "      <td>43.2 centimetre</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260057</th>\n",
       "      <td>263857</td>\n",
       "      <td>510KhVw4VSL.jpg</td>\n",
       "      <td>752266</td>\n",
       "      <td>height</td>\n",
       "      <td>9.1 centimetre</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260058</th>\n",
       "      <td>263858</td>\n",
       "      <td>51lzTNLQ-6S.jpg</td>\n",
       "      <td>416664</td>\n",
       "      <td>height</td>\n",
       "      <td>27.5 centimetre</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>260059 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         index       image_link  group_id  entity_name     entity_value  \\\n",
       "0            0  61I9XdN6OFL.jpg    748919  item_weight       500.0 gram   \n",
       "1            1  71gSRbyXmoL.jpg    916768  item_volume          1.0 cup   \n",
       "2            2  61BZ4zrjZXL.jpg    459516  item_weight       0.709 gram   \n",
       "3            3  612mrlqiI4L.jpg    459516  item_weight       0.709 gram   \n",
       "4            4  617Tl40LOXL.jpg    731432  item_weight   1400 milligram   \n",
       "...        ...              ...       ...          ...              ...   \n",
       "260054  263854  612J1R1xHlL.jpg    558806       height   5.0 centimetre   \n",
       "260055  263855  61Blzh2+28L.jpg    470067       height         8.5 inch   \n",
       "260056  263856  51MsegDL9VL.jpg    204245       height  43.2 centimetre   \n",
       "260057  263857  510KhVw4VSL.jpg    752266       height   9.1 centimetre   \n",
       "260058  263858  51lzTNLQ-6S.jpg    416664       height  27.5 centimetre   \n",
       "\n",
       "       train/valid  \n",
       "0            train  \n",
       "1            train  \n",
       "2            train  \n",
       "3            train  \n",
       "4            train  \n",
       "...            ...  \n",
       "260054       train  \n",
       "260055       train  \n",
       "260056       train  \n",
       "260057       train  \n",
       "260058       train  \n",
       "\n",
       "[260059 rows x 6 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dat.reset_index(inplace=True)\n",
    "train_dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>image_link</th>\n",
       "      <th>group_id</th>\n",
       "      <th>entity_name</th>\n",
       "      <th>entity_value</th>\n",
       "      <th>train/valid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7196</td>\n",
       "      <td>51UnMWmk+WL.jpg</td>\n",
       "      <td>793731</td>\n",
       "      <td>item_weight</td>\n",
       "      <td>450.0 gram</td>\n",
       "      <td>valid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16238</td>\n",
       "      <td>61MlmQnMzBL.jpg</td>\n",
       "      <td>793731</td>\n",
       "      <td>item_weight</td>\n",
       "      <td>101.0 gram</td>\n",
       "      <td>valid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16373</td>\n",
       "      <td>71PswiXNbcL.jpg</td>\n",
       "      <td>793731</td>\n",
       "      <td>item_weight</td>\n",
       "      <td>400.0 gram</td>\n",
       "      <td>valid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>25580</td>\n",
       "      <td>61x5eHpmrNL.jpg</td>\n",
       "      <td>793731</td>\n",
       "      <td>item_weight</td>\n",
       "      <td>150.0 gram</td>\n",
       "      <td>valid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>26858</td>\n",
       "      <td>71Iub8eRH7L.jpg</td>\n",
       "      <td>793731</td>\n",
       "      <td>item_weight</td>\n",
       "      <td>3.0 ounce</td>\n",
       "      <td>valid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3795</th>\n",
       "      <td>263598</td>\n",
       "      <td>41Lp7a25kYL.jpg</td>\n",
       "      <td>108478</td>\n",
       "      <td>height</td>\n",
       "      <td>15.3 inch</td>\n",
       "      <td>valid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3796</th>\n",
       "      <td>263693</td>\n",
       "      <td>612nLFhf3kL.jpg</td>\n",
       "      <td>318766</td>\n",
       "      <td>height</td>\n",
       "      <td>91.0 centimetre</td>\n",
       "      <td>valid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3797</th>\n",
       "      <td>263700</td>\n",
       "      <td>51nCNRn+fcL.jpg</td>\n",
       "      <td>901135</td>\n",
       "      <td>height</td>\n",
       "      <td>24.0 centimetre</td>\n",
       "      <td>valid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3798</th>\n",
       "      <td>263777</td>\n",
       "      <td>51qAAQELGLL.jpg</td>\n",
       "      <td>251430</td>\n",
       "      <td>height</td>\n",
       "      <td>18.0 centimetre</td>\n",
       "      <td>valid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3799</th>\n",
       "      <td>263801</td>\n",
       "      <td>51JORX+FG6L.jpg</td>\n",
       "      <td>966488</td>\n",
       "      <td>height</td>\n",
       "      <td>58.4 centimetre</td>\n",
       "      <td>valid</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3800 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       index       image_link  group_id  entity_name     entity_value  \\\n",
       "0       7196  51UnMWmk+WL.jpg    793731  item_weight       450.0 gram   \n",
       "1      16238  61MlmQnMzBL.jpg    793731  item_weight       101.0 gram   \n",
       "2      16373  71PswiXNbcL.jpg    793731  item_weight       400.0 gram   \n",
       "3      25580  61x5eHpmrNL.jpg    793731  item_weight       150.0 gram   \n",
       "4      26858  71Iub8eRH7L.jpg    793731  item_weight        3.0 ounce   \n",
       "...      ...              ...       ...          ...              ...   \n",
       "3795  263598  41Lp7a25kYL.jpg    108478       height        15.3 inch   \n",
       "3796  263693  612nLFhf3kL.jpg    318766       height  91.0 centimetre   \n",
       "3797  263700  51nCNRn+fcL.jpg    901135       height  24.0 centimetre   \n",
       "3798  263777  51qAAQELGLL.jpg    251430       height  18.0 centimetre   \n",
       "3799  263801  51JORX+FG6L.jpg    966488       height  58.4 centimetre   \n",
       "\n",
       "     train/valid  \n",
       "0          valid  \n",
       "1          valid  \n",
       "2          valid  \n",
       "3          valid  \n",
       "4          valid  \n",
       "...          ...  \n",
       "3795       valid  \n",
       "3796       valid  \n",
       "3797       valid  \n",
       "3798       valid  \n",
       "3799       valid  \n",
       "\n",
       "[3800 rows x 6 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_dat.reset_index(inplace=True)\n",
    "valid_dat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('<MORE_DETAILED_CAPTION>',\n",
       " 'item_weight',\n",
       " '500.0 gram',\n",
       " <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1600x1600>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = FinetuneDataset(train_dat,'train')\n",
    "next(iter(train_dataset))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('<MORE_DETAILED_CAPTION>',\n",
       " 'item_weight',\n",
       " '450.0 gram',\n",
       " <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1010x1010>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_dataset = FinetuneDataset(valid_dat,'valid')\n",
    "next(iter(valid_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    task_prompt, text_prompt, answers, images = zip(*batch)\n",
    "    inputs = processor(text=list(text_prompt), images=list(images), return_tensors=\"pt\", padding=True).to(device)\n",
    "    return inputs, answers\n",
    "batch_size = 1\n",
    "num_workers = 0\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn, num_workers=num_workers)\n",
    "val_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn, num_workers=num_workers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_loader, val_loader, model, processor, epochs=11, lr=1e-6):\n",
    "    optimizer = AdamW(model.parameters(), lr=lr)\n",
    "    num_training_steps = epochs * len(train_loader)\n",
    "    lr_scheduler = get_scheduler(\n",
    "        name=\"linear\",\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=0,\n",
    "        num_training_steps=num_training_steps,\n",
    "    )\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        i = -1\n",
    "        for batch in tqdm(train_loader, desc=f\"Training Epoch {epoch + 1}/{epochs}\"):\n",
    "            i += 1\n",
    "            inputs, answers = batch\n",
    "\n",
    "            input_ids = inputs[\"input_ids\"]\n",
    "            pixel_values = inputs[\"pixel_values\"]\n",
    "            labels = processor.tokenizer(text=answers, return_tensors=\"pt\", padding=True, return_token_type_ids=False).input_ids.to(device)\n",
    "            outputs = model(input_ids=input_ids, pixel_values=pixel_values, labels=labels)\n",
    "            loss = outputs.loss\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        print(f\"Average Training Loss: {avg_train_loss}\")\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(val_loader, desc=f\"Validation Epoch {epoch + 1}/{epochs}\"):\n",
    "                inputs, answers = batch\n",
    "\n",
    "                input_ids = inputs[\"input_ids\"]\n",
    "                pixel_values = inputs[\"pixel_values\"]\n",
    "                labels = processor.tokenizer(text=answers, return_tensors=\"pt\", padding=True, return_token_type_ids=False).input_ids.to(device)\n",
    "\n",
    "                outputs = model(input_ids=input_ids, pixel_values=pixel_values, labels=labels)\n",
    "                loss = outputs.loss\n",
    "\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        print(f\"Average Validation Loss: {avg_val_loss}\")\n",
    "\n",
    "        # Save model checkpoint\n",
    "        output_dir = f\"./model_checkpoints/epoch_{epoch+1}\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        model.save_pretrained(output_dir)\n",
    "        processor.save_pretrained(output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/1:   0%|          | 6/260059 [00:43<527:38:19,  7.30s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m param \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mvision_tower\u001b[38;5;241m.\u001b[39mparameters():\n\u001b[1;32m      3\u001b[0m   param\u001b[38;5;241m.\u001b[39mis_trainable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocessor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[20], line 25\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(train_loader, val_loader, model, processor, epochs, lr)\u001b[0m\n\u001b[1;32m     22\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(input_ids\u001b[38;5;241m=\u001b[39minput_ids, pixel_values\u001b[38;5;241m=\u001b[39mpixel_values, labels\u001b[38;5;241m=\u001b[39mlabels)\n\u001b[1;32m     23\u001b[0m loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n\u001b[0;32m---> 25\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     27\u001b[0m lr_scheduler\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/Enter/lib/python3.9/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Enter/lib/python3.9/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "     \n",
    "# Note: We will freeze image encoder for this tutorial. The authors have reported improvement in unfreezing image encoder, but note that this will result in more resource usage.\n",
    "for param in model.vision_tower.parameters():\n",
    "  param.is_trainable = False\n",
    "  \n",
    "train_model(train_loader, val_loader, model, processor, epochs=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
